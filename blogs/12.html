<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Reinforcement Learning — Reward Oriented Intelligence</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Reinforcement Learning — Reward Oriented Intelligence</h1>
</header>
<section data-field="subtitle" class="p-summary">
Introduction to Reinforcement Learning with deep intuition of Markov Decision Process (MDP), Markov Chains and Discounted Rate γ.
</section>
<section data-field="body" class="e-content">
<section name="442b" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="7a38" id="7a38" class="graf graf--h3 graf--leading graf--title">Reinforcement Learning — Reward Oriented Intelligence</h3><h4 name="495e" id="495e" class="graf graf--h4 graf-after--h3 graf--subtitle">Introduction to Reinforcement Learning with deep intuition of Markov Decision Process (MDP), Markov Chains and Discounted Rate γ.</h4><figure name="973a" id="973a" class="graf graf--figure graf-after--h4"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 525px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 75%;"></div><img class="graf-image" data-image-id="0*xfXU4GmAsb5eywfq.png" data-width="1200" data-height="900" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/0*xfXU4GmAsb5eywfq.png"></div><figcaption class="imageCaption">Source: <a href="https://www.google.com/url?sa=i&amp;url=https%3A%2F%2Fbecominghuman.ai%2Fthe-end-of-business-intelligence-80ee439e7669&amp;psig=AOvVaw3uYlJiQHNx9UsOrX2jySBW&amp;ust=1594209233383000&amp;source=images&amp;cd=vfe&amp;ved=0CAIQjRxqFwoTCOi-sq2Ku-oCFQAAAAAdAAAAABAI" data-href="https://www.google.com/url?sa=i&amp;url=https%3A%2F%2Fbecominghuman.ai%2Fthe-end-of-business-intelligence-80ee439e7669&amp;psig=AOvVaw3uYlJiQHNx9UsOrX2jySBW&amp;ust=1594209233383000&amp;source=images&amp;cd=vfe&amp;ved=0CAIQjRxqFwoTCOi-sq2Ku-oCFQAAAAAdAAAAABAI" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">here</a></figcaption></figure><p name="6a39" id="6a39" class="graf graf--p graf--hasDropCapModel graf--hasDropCap graf-after--figure"><span class="graf-dropCap">As</span> we saw in the above image, we can see the robot is thinking. This is actually Reinforcement Learning, i.e. making computers to learn itself by making various decisions. Let’s look at the <strong class="markup--strong markup--p-strong">definition</strong> part:</p><p name="6cea" id="6cea" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Reinforcement learning</strong> (<strong class="markup--strong markup--p-strong">RL</strong>) is an area of machine learning concerned with how software agents ought to take actions in an environment in order to maximize the notion of cumulative reward. Reinforcement learning is one of three basic machine learning paradigms, alongside supervised learning and unsupervised learning.</p><p name="ff17" id="ff17" class="graf graf--p graf-after--p">Actually, you might find this definition difficulty to understand but don&#39;t worry, even I don&#39;t understand definitions properly. So, let me conclude the definition: Reinforcement Learning is a <a href="https://medium.com/analytics-vidhya/types-processes-of-machine-learning-data-science-part-4-b0d287dc0068" data-href="https://medium.com/analytics-vidhya/types-processes-of-machine-learning-data-science-part-4-b0d287dc0068" class="markup--anchor markup--p-anchor" target="_blank">type of Machine Learning</a>. This learning makes the computer itself to learn from it’s environment, gets reward on successfully completing a task and main aim is to maximize the reward after the end of all tasks. Trough various blogs, I have already completed all supervised and unsupervised Machine Learning algorithms with math intuition, and now it&#39;s time to learn reinforcement learning.</p><p name="54ca" id="54ca" class="graf graf--p graf-after--p">Reinforcement Learning has<strong class="markup--strong markup--p-strong"> great scope</strong> in future, it is said to be the hope of true artificial intelligence. Reinforcement Learning is growing rapidly, producing wide variety of learning algorithms for different applications. Hence it is important to be familiar with the techniques of reinforcement learning.</p><h3 name="7e67" id="7e67" class="graf graf--h3 graf-after--p">Terms in Reinforcement Learning</h3><ol class="postList"><li name="34b2" id="34b2" class="graf graf--li graf-after--h3"><strong class="markup--strong markup--li-strong">Agent.</strong> The program you train to perform specific task is an agent.</li><li name="e218" id="e218" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Environment. </strong>The surrounding (real or virtual) in which the agent performs actions.</li><li name="9bf9" id="9bf9" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Action.</strong> A move made by the agent, which causes a status change in the environment.</li><li name="4e9d" id="4e9d" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Rewards. </strong>The evaluation or score of an action performed by agent, which can be positive or negative.</li></ol><p name="74f0" id="74f0" class="graf graf--p graf-after--li">We can understand this terminology by looking at a <strong class="markup--strong markup--p-strong">reinforced learned robot</strong>, it will surely be <em class="markup--em markup--p-em">interesting</em>.</p><figure name="4210" id="4210" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 366px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 52.300000000000004%;"></div><img class="graf-image" data-image-id="0*EOu_YO8mHSdifPW7.jpg" data-width="1200" data-height="628" src="https://cdn-images-1.medium.com/max/800/0*EOu_YO8mHSdifPW7.jpg"></div><figcaption class="imageCaption">Source: <a href="https://www.google.com/url?sa=i&amp;url=https%3A%2F%2Ffuturism.com%2Fthe-byte%2Ftidying-robot-clean-room&amp;psig=AOvVaw0TJlR58SNz857YJXr7zt5r&amp;ust=1594211277440000&amp;source=images&amp;cd=vfe&amp;ved=0CAIQjRxqFwoTCMjY8PqRu-oCFQAAAAAdAAAAABAV" data-href="https://www.google.com/url?sa=i&amp;url=https%3A%2F%2Ffuturism.com%2Fthe-byte%2Ftidying-robot-clean-room&amp;psig=AOvVaw0TJlR58SNz857YJXr7zt5r&amp;ust=1594211277440000&amp;source=images&amp;cd=vfe&amp;ved=0CAIQjRxqFwoTCMjY8PqRu-oCFQAAAAAdAAAAABAV" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">here</a></figcaption></figure><p name="5411" id="5411" class="graf graf--p graf-after--figure">This is basically a plastic cleaning robot, it’s main aim is to collect plastics garbage from the floor. The robot works this way:</p><ul class="postList"><li name="8272" id="8272" class="graf graf--li graf-after--p">Gets <strong class="markup--strong markup--li-strong">+10</strong> points when it successfully pick a plastic.</li><li name="a238" id="a238" class="graf graf--li graf-after--li">Gets<strong class="markup--strong markup--li-strong"> -10</strong> when it hits a person.</li><li name="b2ca" id="b2ca" class="graf graf--li graf-after--li">Gets <strong class="markup--strong markup--li-strong">-50</strong> when it falls off.</li><li name="787e" id="787e" class="graf graf--li graf-after--li">Gets <strong class="markup--strong markup--li-strong">+50</strong> when it successfully collect all the garbage in desired time.</li></ul><p name="fb93" id="fb93" class="graf graf--p graf-after--li">Here our <strong class="markup--strong markup--p-strong">Robot </strong>is<strong class="markup--strong markup--p-strong"> Agent, </strong>room’s<strong class="markup--strong markup--p-strong"> Floor </strong>is<strong class="markup--strong markup--p-strong"> Environment, Pick garbage </strong>is <strong class="markup--strong markup--p-strong">Action </strong>and<strong class="markup--strong markup--p-strong"> Points earned </strong>is <strong class="markup--strong markup--p-strong">Rewards.</strong></p><blockquote name="a061" id="a061" class="graf graf--blockquote graf-after--p">So, till now we are done with the basic definition and the terminology.</blockquote><h3 name="353d" id="353d" class="graf graf--h3 graf-after--blockquote">Characteristics of Reinforcement Learning(RL)</h3><p name="f386" id="f386" class="graf graf--p graf-after--h3"><strong class="markup--strong markup--p-strong">RL does exploitation</strong>: RL main aim is to maximize it’s rewards, thus exploitation is it’s main aim.</p><p name="651e" id="651e" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">RL is dynamic in nature</strong>: RL has no specific answer (output), it’s action depends on it’s surrounding. Most of the time, RL takes action on basis of trail and error method, thus being dynamic.</p><p name="bdf5" id="bdf5" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">RL requires exploration</strong>: RL needs to get information about it’s environment by exploration to perform better. Generally, RL needs to balance between exploration and exploitation.</p><p name="3769" id="3769" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">RL is multi-decision system</strong>: RL forms a decision chain to perform a given task, thus taking multiple decision at each stage in completion of work.</p><p name="3d7b" id="3d7b" class="graf graf--p graf-after--p">These were the base characteristics of RL, I hope you got it well. Further, in the blog we will discuss the decision process of RL.</p><h3 name="1bbd" id="1bbd" class="graf graf--h3 graf-after--p">Introducing the Markov Decision Process (MDP)</h3><p name="f648" id="f648" class="graf graf--p graf-after--h3">Markov decision processes give us a way to formalize sequential decision making. This formalization is the basis for structuring problems that are solved with reinforcement learning. This can be designed as:</p><ul class="postList"><li name="60cf" id="60cf" class="graf graf--li graf-after--p">Set of states, S</li><li name="8f02" id="8f02" class="graf graf--li graf-after--li">Set of actions, A</li><li name="2f48" id="2f48" class="graf graf--li graf-after--li">Reward function, R</li><li name="a3ed" id="a3ed" class="graf graf--li graf-after--li">Policy, π</li><li name="7cc8" id="7cc8" class="graf graf--li graf-after--li">Value, V</li></ul><p name="37ac" id="37ac" class="graf graf--p graf-after--li">This process of selecting an action(<strong class="markup--strong markup--p-strong">A</strong>) from a given state(<strong class="markup--strong markup--p-strong">S1</strong>), transitioning to a new state(<strong class="markup--strong markup--p-strong">S2</strong>), and receiving a reward(<strong class="markup--strong markup--p-strong">R1</strong>) happens sequentially over and over again, which creates something called a <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">trajectory</em></strong> that shows the sequence of states, actions, and rewards.</p><p name="2d72" id="2d72" class="graf graf--p graf-after--p">The set of actions we took define our policy (<strong class="markup--strong markup--p-strong">π</strong>) and the rewards we get in return defines our value (<strong class="markup--strong markup--p-strong">V</strong>). Our task here is to maximize our rewards by choosing the correct policy.</p><p name="8300" id="8300" class="graf graf--p graf-after--p"><em class="markup--em markup--p-em">This diagram nicely illustrates this entire idea.</em></p><figure name="97f6" id="97f6" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 254px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 36.199999999999996%;"></div><img class="graf-image" data-image-id="0*Lx1ou0GBYTL9klv4.png" data-width="1068" data-height="387" src="https://cdn-images-1.medium.com/max/800/0*Lx1ou0GBYTL9klv4.png"></div><figcaption class="imageCaption">Image credit: <a href="https://www.google.com/url?sa=i&amp;url=https%3A%2F%2Fwww.researchgate.net%2Ffigure%2FRL-Environment-Interaction-16_fig1_306056598&amp;psig=AOvVaw3_j2nuw5JNQe58NanWAhEe&amp;ust=1594227502873000&amp;source=images&amp;cd=vfe&amp;ved=0CAIQjRxqFwoTCKC_i7POu-oCFQAAAAAdAAAAABAD" data-href="https://www.google.com/url?sa=i&amp;url=https%3A%2F%2Fwww.researchgate.net%2Ffigure%2FRL-Environment-Interaction-16_fig1_306056598&amp;psig=AOvVaw3_j2nuw5JNQe58NanWAhEe&amp;ust=1594227502873000&amp;source=images&amp;cd=vfe&amp;ved=0CAIQjRxqFwoTCKC_i7POu-oCFQAAAAAdAAAAABAD" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">here</a></figcaption></figure><p name="b809" id="b809" class="graf graf--p graf-after--figure">Let’s break down this diagram into steps.</p><ol class="postList"><li name="4e16" id="4e16" class="graf graf--li graf-after--p">At time T1, the environment is in it’s initial state S1.</li><li name="aa55" id="aa55" class="graf graf--li graf-after--li">The agent observes the current state and selects action A1.</li><li name="19c1" id="19c1" class="graf graf--li graf-after--li">The action the environ state from S1 to S2.</li><li name="2a89" id="2a89" class="graf graf--li graf-after--li">The environment transitions to state and grants the agent reward R1.</li><li name="c2c0" id="c2c0" class="graf graf--li graf-after--li">This process then starts over for the next time step T2, now with initial state S2 .</li></ol><p name="326a" id="326a" class="graf graf--p graf-after--li">Note: This process continues until the final goal is reached. At each stage, a new state is formed and a new decision is taken, finally forming a <strong class="markup--strong markup--p-strong">Markov Chain</strong>.</p><blockquote name="246b" id="246b" class="graf graf--pullquote graf-after--p">A <strong class="markup--strong markup--pullquote-strong">Markov Chain</strong> is a stochastic model describing a sequence of possible events in which the probability of each event depends only on the state attained in the previous event.</blockquote><p name="6a15" id="6a15" class="graf graf--p graf-after--pullquote">P1(S2,R1 ; S1,A1) = Probability{(S(t+1)=S2, R(t+1)=R1) / (S(t)=S1, A(t)=A1} (t is initial time)</p><p name="615e" id="615e" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Expected Return (G)</strong>: The agent goal is to maximize the expected return, or the <strong class="markup--strong markup--p-strong">sum</strong> of all returns.</p><p name="9395" id="9395" class="graf graf--p graf-after--p">G = R1 + R2 + R3 + ….. R(T, final timestep)</p><h3 name="4d14" id="4d14" class="graf graf--h3 graf-after--p">Episodic vs continuous tasks</h3><p name="2241" id="2241" class="graf graf--p graf-after--h3">Episodic tasks are the tasks that have a terminal state (end). In RL, episodes are considered agent-environment interactions from initial to final states.</p><p name="3217" id="3217" class="graf graf--p graf-after--p">For example, in a car racing video game, you start the game (initial state) and play the game until it is over (final state). This is called an episode. Once the game is over, you start the next episode by restarting the game, and you will begin from the initial state irrespective of the position you were in the previous game. So, each episode is independent of the other.</p><p name="8635" id="8635" class="graf graf--p graf-after--p">In a continuous task, there is not a terminal state. Continuous tasks will never end. For example, a personal assistance robot does not have a terminal state.</p><p name="348b" id="348b" class="graf graf--p graf-after--p">Now, in order to focus on the maximizing the return, we need to take care how current action will affect the future. For that we introduce discounted rate.</p><h3 name="f093" id="f093" class="graf graf--h3 graf-after--p">Discounted Return</h3><p name="6fbd" id="6fbd" class="graf graf--p graf-after--h3">Rather than the agent’s goal being to maximize the expected return of rewards, it will instead be to maximize the expected <em class="markup--em markup--p-em">discounted</em> return of rewards.</p><p name="70f4" id="70f4" class="graf graf--p graf-after--p">The <strong class="markup--strong markup--p-strong">discount</strong> factor essentially determines how much the <strong class="markup--strong markup--p-strong">reinforcement learning</strong> agents cares about <strong class="markup--strong markup--p-strong">rewards</strong> in the distant future relative to those in the immediate future. If γ=0, the agent will be completely myopic and only <strong class="markup--strong markup--p-strong">learn</strong> about actions that produce an immediate <strong class="markup--strong markup--p-strong">reward</strong>. The value of γ usually lies between 0 and 1, i.e. [0, 1].</p><p name="b412" id="b412" class="graf graf--p graf-after--p">Thus the value of G changes as follows:</p><p name="cab6" id="cab6" class="graf graf--p graf-after--p">G = R1 + γR2 + γ²R3 + γ³R4 + ….</p><p name="95ef" id="95ef" class="graf graf--p graf-after--p">G = R1 + γ(R2 + γR3 + γ²R4 + ….)</p><p name="e9d9" id="e9d9" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">G = R1 + γG1 </strong>(G1 <strong class="markup--strong markup--p-strong">= </strong>1/(1-γ))</p><p name="9cad" id="9cad" class="graf graf--p graf-after--p">This infinite sum yields a finite result. This is same as infinite sum of Geometric Progression, i.e. 1/(1-γ).</p><p name="b6d4" id="b6d4" class="graf graf--p graf-after--p">While the agent does consider all of the expected future rewards when selecting an action, the more immediate rewards influence the agent greater than rewards that are expected to be received further out due to the discount rate.</p><h3 name="b324" id="b324" class="graf graf--h3 graf-after--p">Summary</h3><ul class="postList"><li name="c671" id="c671" class="graf graf--li graf-after--h3">First, we discussed the meaning of Reinforcement Learning.</li><li name="aa9f" id="aa9f" class="graf graf--li graf-after--li">We learned major terminology with an example.</li><li name="b147" id="b147" class="graf graf--li graf-after--li">Saw the characteristics of RL.</li><li name="6e58" id="6e58" class="graf graf--li graf-after--li">Then completed the major part of Markov decision process.</li><li name="3b48" id="3b48" class="graf graf--li graf-after--li">Finally, ended up with the discounted rate.</li></ul><p name="3a9f" id="3a9f" class="graf graf--p graf-after--li graf--trailing">Next time we’ll be building on the ideas from our introduction to MDPs and discounted return to see how we can measure “ <em class="markup--em markup--p-em">how good</em>” any particular state or any particular action is for the agent with a given policy. I’ll see ya <a href="https://medium.com/@deeppatel23/reinforcement-learning-optimal-policy-%CF%80-7b7592c4fd79" data-href="https://medium.com/@deeppatel23/reinforcement-learning-optimal-policy-%CF%80-7b7592c4fd79" class="markup--anchor markup--p-anchor" target="_blank">in the next one</a>!</p></div></div></section>
</section><!--
<footer><p>By <a href="https://medium.com/@deeppatel23" class="p-author h-card">Deep Patel</a> on <a href="https://medium.com/p/66a20e3b8edb"><time class="dt-published" datetime="2020-07-07T17:00:39.222Z">July 7, 2020</time></a>.</p><p><a href="https://medium.com/@deeppatel23/reinforcement-learning-reward-oriented-intelligence-66a20e3b8edb" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on September 4, 2020.</p></footer>--></article></body></html>