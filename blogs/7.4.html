<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Classifying via Bayes Theorem | Naïve Bayes Classifier</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Classifying via Bayes Theorem | Naïve Bayes Classifier</h1>
</header>
<section data-field="subtitle" class="p-summary">
Naïve Bayes classifier totally work on Bayes Theorem. So first go through it.
</section>
<section data-field="body" class="e-content">
<section name="8237" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="93e0" id="93e0" class="graf graf--h3 graf--leading graf--title">Naïve Bayes Classifier | Data Science | ML (Part 7.4)</h3><h4 name="d3a6" id="d3a6" class="graf graf--h4 graf-after--h3 graf--subtitle">Naïve Bayes classifier totally work on <strong class="markup--strong markup--h4-strong">Bayes Theorem</strong>. So first go through it.</h4><figure name="ecdd" id="ecdd" class="graf graf--figure graf-after--h4"><div class="aspectRatioPlaceholder is-locked" style="max-width: 633px; max-height: 436px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 68.89999999999999%;"></div><img class="graf-image" data-image-id="1*SRfrqh4bOqYlMdpmhIetqg.jpeg" data-width="633" data-height="436" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/1*SRfrqh4bOqYlMdpmhIetqg.jpeg"></div><figcaption class="imageCaption">Tribute to Great Actor (Bae is just a joke) Source:<a href="https://ifunny.co/picture/bayes-theorem-ts-likelihood-bae-s-theorem-p-netflixichill-p-07kztVNJ7" data-href="https://ifunny.co/picture/bayes-theorem-ts-likelihood-bae-s-theorem-p-netflixichill-p-07kztVNJ7" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">https://ifunny.co/picture/bayes-theorem-ts-likelihood-bae-s-theorem-p-netflixichill-p-07kztVNJ7</a></figcaption></figure><p name="3682" id="3682" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">Bayes’ theorem</strong> is a formula that describes how to update the probabilities of hypotheses when given evidence. It follows simply from the axioms of conditional probability, but can be used to powerfully reason about a wide range of problems involving belief updates. Conditional probability is the probability of one thing being true given that another thing is true, and is the key concept in Bayes’ theorem.</p><p name="50ea" id="50ea" class="graf graf--p graf-after--p">Given a hypothesis H and evidence E, Bayes’ theorem states that the relationship between the probability of the hypothesis before getting the evidence P(H) and the probability of the hypothesis after getting the evidence P(H \E) is —</p><figure name="35b1" id="35b1" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 333px; max-height: 74px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 22.2%;"></div><img class="graf-image" data-image-id="1*ZsMzkjd1VN_op1248IM3tQ.png" data-width="333" data-height="74" src="https://cdn-images-1.medium.com/max/800/1*ZsMzkjd1VN_op1248IM3tQ.png"></div><figcaption class="imageCaption">Formula</figcaption></figure><p name="ad2b" id="ad2b" class="graf graf--p graf-after--figure">Many modern machine learning techniques rely on Bayes’ theorem. For instance, spam filters use Bayesian updating to determine whether an email is real or spam, given the words in the email. Additionally, many specific techniques in statistics, such as calculating p-values or interpreting medical results, are best described in terms of how they contribute to updating hypotheses using Bayes’ theorem.</p><p name="50ff" id="50ff" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Naïve Bayes</strong> is a probabilistic algorithm that’s typically used for classification problems. Naïve Bayes is simple, intuitive, and yet performs surprisingly well in many cases. For example, spam filters Email app uses are built on Naïve Bayes.</p><p name="4374" id="4374" class="graf graf--p graf-after--p">Naïve Bayes is a statistical classification technique based on Bayes Theorem. It is one of the simplest supervised learning algorithms. Naïve Bayes classifier is the fast, accurate and reliable algorithm. Naïve Bayes classifiers have high accuracy and speed on large datasets.</p><p name="db0b" id="db0b" class="graf graf--p graf-after--p">Dataset works on BAYES Theorem in similar pattern:</p><p name="a8d7" id="a8d7" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">P(y / f1f2f3…fn) = {P(f1/y).P(f2/y)…P(fn/y) * P(y)} / P(f1)…P(fn)</strong></p><p name="eef4" id="eef4" class="graf graf--p graf-after--p">here, y is the label (output) and f(1,2…n) is list of features,</p><p name="cbb1" id="cbb1" class="graf graf--p graf-after--p">P(yes) = P(yes/fn) / {P(yes/fn) + P(no/fn)}</p><p name="c4cc" id="c4cc" class="graf graf--p graf-after--p">Still didn&#39;t understood the working, Don&#39;t worry, see following <strong class="markup--strong markup--p-strong">example</strong>:</p><p name="8a93" id="8a93" class="graf graf--p graf-after--p">Given an example of weather conditions and playing sports. You need to calculate the probability of playing sports. Now, you need to classify whether players will play or not, based on the weather condition.</p><p name="de86" id="de86" class="graf graf--p graf-after--p">Below I have a training data set of weather and corresponding target variable ‘Play’ (suggesting possibilities of playing). Now, we need to classify whether players will play or not based on weather condition. Let’s follow the below steps to perform it.</p><ul class="postList"><li name="2ccb" id="2ccb" class="graf graf--li graf-after--p">Step 1: Calculate the <strong class="markup--strong markup--li-strong">prior</strong> probability for given class labels</li><li name="0706" id="0706" class="graf graf--li graf-after--li">Step 2: Find <strong class="markup--strong markup--li-strong">Likelihood</strong> probability with each attribute for each class</li><li name="ce8c" id="ce8c" class="graf graf--li graf-after--li">Step 3: Put these value in Bayes Formula and calculate <strong class="markup--strong markup--li-strong">posterior</strong> probability.</li><li name="9d67" id="9d67" class="graf graf--li graf-after--li">Step 4: See which class has a higher probability, given the input belongs to the higher probability class.</li></ul><figure name="9d1b" id="9d1b" class="graf graf--figure graf-after--li"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 255px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 36.5%;"></div><img class="graf-image" data-image-id="0*TF59n1x4HTiRn0zO.png" data-width="861" data-height="314" src="https://cdn-images-1.medium.com/max/800/0*TF59n1x4HTiRn0zO.png"></div><figcaption class="imageCaption">Dataset</figcaption></figure><p name="5eb2" id="5eb2" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">Problem: </strong>Players will play if weather is sunny. Is this statement is correct?</p><p name="1b4f" id="1b4f" class="graf graf--p graf-after--p">We can solve it using above discussed method of posterior probability.</p><p name="ea77" id="ea77" class="graf graf--p graf-after--p">P(Yes | Sunny) = P( Sunny | Yes) * P(Yes) / P (Sunny)</p><p name="2375" id="2375" class="graf graf--p graf-after--p">Here we have P (Sunny |Yes) = 3/9 = 0.33, P(Sunny) = 5/14 = 0.36, P( Yes)= 9/14 = 0.64</p><p name="58dd" id="58dd" class="graf graf--p graf-after--p">Now, P (Yes | Sunny) = 0.33 * 0.64 / 0.36 = 0.60, which has higher probability.</p><p name="1e57" id="1e57" class="graf graf--p graf-after--p">Naïve Bayes uses a similar method to predict the probability of different class based on various attributes. This algorithm is mostly used in text classification and with problems having multiple classes.</p><h3 name="dacf" id="dacf" class="graf graf--h3 graf-after--p">How to build a basic model using Naïve Bayes in Python ?</h3><p name="0143" id="0143" class="graf graf--p graf-after--h3">Again, scikit learn (python library) will help here to build a Naïve Bayes model in Python. There are three types of Naïve Bayes model under the scikit-learn library:</p><ul class="postList"><li name="83c8" id="83c8" class="graf graf--li graf-after--p"><a href="http://scikit-learn.org/stable/modules/naive_bayes.html" data-href="http://scikit-learn.org/stable/modules/naive_bayes.html" class="markup--anchor markup--li-anchor" rel="nofollow noopener noreferrer noopener" target="_blank"><strong class="markup--strong markup--li-strong">Gaussian:</strong></a><strong class="markup--strong markup--li-strong"> </strong>It is used in classification and it assumes that features follow a normal distribution.</li><li name="a11a" id="a11a" class="graf graf--li graf-after--li"><a href="http://scikit-learn.org/stable/modules/naive_bayes.html" data-href="http://scikit-learn.org/stable/modules/naive_bayes.html" class="markup--anchor markup--li-anchor" rel="nofollow noopener noreferrer noopener" target="_blank"><strong class="markup--strong markup--li-strong">Multinomial</strong></a><strong class="markup--strong markup--li-strong">: </strong>It is used for discrete counts. For example, let’s say, we have a text classification problem. Here we can consider Bernoulli trials which is one step further and instead of “word occurring in the document”, we have “count how often word occurs in the document”, you can think of it as “number of times outcome number x_i is observed over the n trials”.</li><li name="73fc" id="73fc" class="graf graf--li graf-after--li"><a href="http://scikit-learn.org/stable/modules/naive_bayes.html" data-href="http://scikit-learn.org/stable/modules/naive_bayes.html" class="markup--anchor markup--li-anchor" rel="nofollow noopener noreferrer noopener" target="_blank"><strong class="markup--strong markup--li-strong">Bernoulli</strong></a><strong class="markup--strong markup--li-strong">: </strong>The binomial model is useful if your feature vectors are binary (i.e. zeros and ones). One application would be text classification with ‘bag of words’(Natural Language Processing algorithm will be covered in future blogs) model where the 1s &amp; 0s are “word occurs in the document” and “word does not occur in the document” respectively.</li></ul><pre name="6c4b" id="6c4b" class="graf graf--pre graf-after--li"># importing required libraries</pre><pre name="c918" id="c918" class="graf graf--pre graf-after--pre">import pandas as pd</pre><pre name="7bd7" id="7bd7" class="graf graf--pre graf-after--pre">from sklearn.naive_bayes import <strong class="markup--strong markup--pre-strong">GaussianNB</strong><br>from sklearn.naive_bayes import <strong class="markup--strong markup--pre-strong">MultinomialNB<br></strong>from sklearn.naive_bayes import <strong class="markup--strong markup--pre-strong">BernoulliNB</strong><br><br>from sklearn.metrics import accuracy_score</pre><pre name="aa10" id="aa10" class="graf graf--pre graf-after--pre">#You must select model according to the need of dataset<br>model = <strong class="markup--strong markup--pre-strong">GaussianNB()</strong><br>clf = <strong class="markup--strong markup--pre-strong">MultinomialNB()</strong><br>clf = <strong class="markup--strong markup--pre-strong">BernoulliNB()</strong></pre><pre name="914f" id="914f" class="graf graf--pre graf-after--pre"># fit the model with the training data<br>model.fit(train_x,train_y)</pre><pre name="bcd9" id="bcd9" class="graf graf--pre graf-after--pre"># predict the target on the train dataset<br>predict_train = model.predict(train_x)<br>print(&#39;Target on train data&#39;,predict_train)</pre><pre name="5fd0" id="5fd0" class="graf graf--pre graf-after--pre"># Accuray Score on train dataset<br>accuracy_train = accuracy_score(train_y,predict_train)<br>print(&#39;accuracy_score on train dataset : &#39;, accuracy_train)</pre><pre name="e511" id="e511" class="graf graf--pre graf-after--pre"># predict the target on the test dataset<br>predict_test = model.predict(test_x)<br>print(&#39;Target on test data&#39;,predict_test)</pre><pre name="1dc2" id="1dc2" class="graf graf--pre graf-after--pre"># Accuracy Score on test dataset<br>accuracy_test = accuracy_score(test_y,predict_test)<br>print(&#39;accuracy_score on test dataset : &#39;, accuracy_test)</pre><h3 name="daff" id="daff" class="graf graf--h3 graf-after--pre">Tips to improve the power of Naïve Bayes Model</h3><p name="f625" id="f625" class="graf graf--p graf-after--h3">Here are some tips for improving power of Naïve Bayes Model:</p><ul class="postList"><li name="00e0" id="00e0" class="graf graf--li graf-after--p">If continuous features do not have normal distribution, we should use transformation or different methods to convert it in normal distribution.</li><li name="f31f" id="f31f" class="graf graf--li graf-after--li">If test data set has <strong class="markup--strong markup--li-strong">zero frequency issue</strong>, apply smoothing techniques “<strong class="markup--strong markup--li-strong">Laplace Correction</strong>” to predict the class of test data set. Laplacian correction is one of the <strong class="markup--strong markup--li-strong">smoothing</strong> techniques. Here, you can assume that the dataset is large enough that adding one row of each class will not make a difference in the estimated probability. This will overcome the issue of probability values to zero.</li><li name="6072" id="6072" class="graf graf--li graf-after--li">Remove correlated features, as the highly correlated features are voted twice in the model and it can lead to over inflating importance.</li><li name="bc16" id="bc16" class="graf graf--li graf-after--li">Naïve Bayes classifiers has limited options for parameter tuning like alpha=1 for smoothing, fit_prior=[True / False] to learn class prior probabilities or not and some other options (look at detail here). I would recommend to focus on your pre-processing of data and the feature selection.</li><li name="6d7b" id="6d7b" class="graf graf--li graf-after--li">You might think to apply some <em class="markup--em markup--li-em">classifier combination technique like </em>ensemble, bagging and boosting but these methods would not help. Actually, “ensemble, boosting, bagging” won’t help since their purpose is to reduce variance. Naïve Bayes has no variance to minimize.</li></ul><h3 name="da5d" id="da5d" class="graf graf--h3 graf-after--li">Conclusion</h3><p name="e609" id="e609" class="graf graf--p graf-after--h3">Congratulations, you have made it to the end of this tutorial!</p><p name="1a49" id="1a49" class="graf graf--p graf-after--p">In this tutorial, you learned about Naïve Bayes algorithm, it’s working, Naïve Bayes assumption, issues, implementation, advantages, and disadvantages. Along the road, you have also learned model building and evaluation in scikit-learn for binary and multinomial classes.</p><p name="b2d5" id="b2d5" class="graf graf--p graf-after--p">Naïve Bayes is the most straightforward and most potent algorithm. In spite of the significant advances of Machine Learning in the last couple of years, it has proved its worth. It has been successfully deployed in many applications from text analytics to recommendation engines.</p><p name="5766" id="5766" class="graf graf--p graf-after--p graf--trailing"><strong class="markup--strong markup--p-strong">Thank You</strong> for reading this blog. I hope you have enjoyed learning new algorithm. Make sure you <strong class="markup--strong markup--p-strong">follow</strong> me for more blogs on this topic. <strong class="markup--strong markup--p-strong">Cheers!</strong> Have a Great Day.</p></div></div></section>
</section><!--
<footer><p>By <a href="https://medium.com/@deeppatel23" class="p-author h-card">Deep Patel</a> on <a href="https://medium.com/p/96fd8b29bb28"><time class="dt-published" datetime="2020-06-29T14:05:02.647Z">June 29, 2020</time></a>.</p><p><a href="https://medium.com/@deeppatel23/na%C3%AFve-bayes-classifier-data-science-ml-part-7-4-96fd8b29bb28" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on September 4, 2020.</p></footer>--></article></body></html>