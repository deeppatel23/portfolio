<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Using Trees to Make Decision | Decision Tree Classifier</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Using Trees to Make Decision | Decision Tree Classifier</h1>
</header>
<section data-field="subtitle" class="p-summary">
“All our wisdom is stored in the trees” — Santosh Kalwar
</section>
<section data-field="body" class="e-content">
<section name="d981" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="6cce" id="6cce" class="graf graf--h3 graf--leading graf--title">Decision Tree Classifier | Data Science | ML(Part 7.2)</h3><p name="0c2d" id="0c2d" class="graf graf--p graf-after--h3">Starting the blog with Decision Tree Algorithm. “All our wisdom is stored in the trees<strong class="markup--strong markup--p-strong">”</strong> — Santosh Kalwar</p><figure name="2e78" id="2e78" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 690px; max-height: 381px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 55.2%;"></div><img class="graf-image" data-image-id="0*zz28t0uGGcSSLl5u.png" data-width="690" data-height="381" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/0*zz28t0uGGcSSLl5u.png"></div><figcaption class="imageCaption">Working of Decision Tree</figcaption></figure><p name="cd0d" id="cd0d" class="graf graf--p graf-after--figure">Above example depicts the example of how we take a decision by looking at different parameters in our life. Same steps is followed in decision tree classifier.</p><h3 name="40a2" id="40a2" class="graf graf--h3 graf-after--p">Introduction</h3><p name="2252" id="2252" class="graf graf--p graf-after--h3">A Decision Tree is a simple representation for classifying examples. It is a Supervised Machine Learning where the data is continuously split according to a certain parameter.</p><p name="81cb" id="81cb" class="graf graf--p graf-after--p">Decision tree is the most powerful and popular tool for classification and prediction. A Decision tree is a flowchart like tree structure, where each internal node denotes a test on an attribute, each branch represents an outcome of the test, and each leaf node (terminal node) holds a class label.</p><p name="980c" id="980c" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Decision tree consist of</strong> :</p><ul class="postList"><li name="46ae" id="46ae" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Root</strong>: It is topmost node in a decision tree. It learns to partition on the basis of the attribute value. It partitions the tree in recursively manner call recursive partitioning.</li><li name="7f02" id="7f02" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Branch / Sub-Tree: </strong>A sub section of decision tree is called branch or sub-tree. It connects two nodes of the tree.</li><li name="fa5e" id="fa5e" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Decision Node: </strong>When a sub-node splits into further sub-nodes, then it is called decision node.</li><li name="c046" id="c046" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Leaf/ Terminal Node: </strong>Nodes with no children (no further split) is called Leaf or Terminal node.</li></ul><h3 name="1f9a" id="1f9a" class="graf graf--h3 graf-after--li">How does the Decision Tree algorithm work?</h3><figure name="1659" id="1659" class="graf graf--figure graf-after--h3"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 295px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 42.199999999999996%;"></div><img class="graf-image" data-image-id="0*MoubUgXFnEJd2Gxj.png" data-width="725" data-height="306" src="https://cdn-images-1.medium.com/max/800/0*MoubUgXFnEJd2Gxj.png"></div><figcaption class="imageCaption">Working Algorithm</figcaption></figure><p name="62ad" id="62ad" class="graf graf--p graf-after--figure">The basic idea behind any decision tree algorithm is as follows:</p><ol class="postList"><li name="5d22" id="5d22" class="graf graf--li graf-after--p">Select the best attribute(feature) using Attribute Selection Measures(<strong class="markup--strong markup--li-strong">ASM</strong>) to split the records.</li><li name="d93c" id="d93c" class="graf graf--li graf-after--li">Make that attribute a decision node and breaks the dataset into smaller subsets.</li><li name="e71e" id="e71e" class="graf graf--li graf-after--li">Starts tree building by repeating this process recursively for each child until one of the condition will match:</li></ol><ul class="postList"><li name="89e7" id="89e7" class="graf graf--li graf-after--li">All the tuples belong to the same attribute value.</li><li name="30f5" id="30f5" class="graf graf--li graf-after--li">There are no more remaining attributes.</li><li name="5703" id="5703" class="graf graf--li graf-after--li">There are no more instances.</li></ul><p name="a976" id="a976" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">Before diving deeper into the concept lets understand about impurity, types of measures of impurity. It will help us to understand the algorithm better.</strong></p><p name="c23a" id="c23a" class="graf graf--p graf-after--p">Impurity, as it’s name suggest, it is a mixture of two or more things (like heterogenous), instead of a single component (like homogenous).</p><figure name="1863" id="1863" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 394px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 56.3%;"></div><img class="graf-image" data-image-id="0*hAHDGLT-BPuOz5P1" data-width="960" data-height="540" src="https://cdn-images-1.medium.com/max/800/0*hAHDGLT-BPuOz5P1"></div><figcaption class="imageCaption">Credit: <a href="https://www.datasciencecentral.com/" data-href="https://www.datasciencecentral.com/" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">https://www.datasciencecentral.com</a></figcaption></figure><p name="d85d" id="d85d" class="graf graf--p graf-after--figure">As seen in the figure, the raw form of data that we will get is totally impure data(Fig A). Creating node by nodes(Fig B), we try to make it pure at each level and finally we categorize it(Fig C).</p><p name="b5fe" id="b5fe" class="graf graf--p graf-after--p">There are couple of impurity measures are there, but in this story we will talk about only two such measure,</p><ol class="postList"><li name="d5ba" id="d5ba" class="graf graf--li graf-after--p">Entropy</li><li name="286e" id="286e" class="graf graf--li graf-after--li">Gini index/ Gini impurity</li></ol><h3 name="8673" id="8673" class="graf graf--h3 graf-after--li">Entropy</h3><p name="d90e" id="d90e" class="graf graf--p graf-after--h3">Entropy is amount of information is needed to accurately describe the some sample. So if sample is homogeneous (pure), means all the element are similar than Entropy is 0, else if sample is equally divided than entropy is maximum 1.</p><figure name="cc3d" id="cc3d" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 288px; max-height: 66px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 22.900000000000002%;"></div><img class="graf-image" data-image-id="0*SM2Tqky8PMeKKMGc.png" data-width="288" data-height="66" src="https://cdn-images-1.medium.com/max/800/0*SM2Tqky8PMeKKMGc.png"></div><figcaption class="imageCaption">Entropy Calculation (i is no. of cases)</figcaption></figure><h3 name="c8b3" id="c8b3" class="graf graf--h3 graf-after--figure">Gini index / Gini impurity</h3><p name="afb0" id="afb0" class="graf graf--p graf-after--h3">Gini index is measure of inequality in sample. It has value between 0 and 1. Gini index of value 0 means sample are perfectly homogeneous(pure) and all element are similar, whereas, Gini index of value 1 means maximal inequality among elements(impure). It is sum of the square of the probabilities of each class. It is illustrated as,</p><figure name="0a61" id="0a61" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 270px; max-height: 97px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 35.9%;"></div><img class="graf-image" data-image-id="0*DHiV2qRfUGNv7r17.png" data-width="270" data-height="97" src="https://cdn-images-1.medium.com/max/800/0*DHiV2qRfUGNv7r17.png"></div><figcaption class="imageCaption">Formula for Gini index</figcaption></figure><h3 name="154f" id="154f" class="graf graf--h3 graf-after--figure">Construction of Decision Tree:</h3><p name="6ffa" id="6ffa" class="graf graf--p graf-after--h3"><em class="markup--em markup--p-em">Step 1</em>: Calculate entropy of the target.</p><p name="479c" id="479c" class="graf graf--p graf-after--p"><em class="markup--em markup--p-em">Step 2</em>: The dataset is then split on the different attributes. The entropy for each branch is calculated. Then it is added proportionally, to get total entropy for the split. The resulting entropy is subtracted from the entropy before the split. The result is the Information Gain, or decrease in entropy.</p><p name="4da8" id="4da8" class="graf graf--p graf-after--p"><em class="markup--em markup--p-em">Step 3</em>: Choose attribute with the largest information gain as the decision node, divide the dataset by its branches and repeat the same process on every branch.</p><p name="5d52" id="5d52" class="graf graf--p graf-after--p"><em class="markup--em markup--p-em">Step 4a</em>: A branch with entropy of 0 is a leaf node.</p><p name="fe31" id="fe31" class="graf graf--p graf-after--p"><em class="markup--em markup--p-em">Step 4b</em>: A branch with entropy more than 0 needs further splitting.</p><p name="f3bb" id="f3bb" class="graf graf--p graf-after--p">Now, we have learned almost all the necessary intuition behind Decision-Tree Algorithm, so let’s see it’s code.</p><pre name="8069" id="8069" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong"># Importing important libraries</strong> <br>from sklearn.tree import DecisionTreeClassifier<br>from sklearn.metrics import accuracy_score</pre><pre name="ab02" id="ab02" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong"># Make a model</strong> <br>model = DecisionTreeClassifier()</pre><pre name="a837" id="a837" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong"># fit the model with the training data</strong><br>model.fit(train_x,train_y)</pre><pre name="f917" id="f917" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong"># depth of the decision tree</strong><br>print(&#39;Depth of the Decision Tree :&#39;, model.get_depth())</pre><pre name="86bc" id="86bc" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong"># predict the target on the train dataset</strong><br>predict_train = model.predict(train_x)<br>print(&#39;Target on train data&#39;,predict_train)</pre><pre name="bdf9" id="bdf9" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong"># Accuray Score on train dataset</strong><br>accuracy_train = accuracy_score(train_y,predict_train)<br>print(&#39;accuracy_score on train dataset : &#39;, accuracy_train)</pre><pre name="9121" id="9121" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong"># predict the target on the test dataset</strong><br>predict_test = model.predict(test_x)<br>print(&#39;Target on test data&#39;,predict_test)</pre><pre name="c6fd" id="c6fd" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong"># Accuracy Score on test dataset</strong><br>accuracy_test = accuracy_score(test_y,predict_test)<br>print(&#39;accuracy_score on test dataset : &#39;, accuracy_test)</pre><h3 name="0150" id="0150" class="graf graf--h3 graf-after--pre">Decision Tree — Overfitting</h3><p name="c0b6" id="c0b6" class="graf graf--p graf-after--h3">Overfitting is a significant practical difficulty for decision tree models and many other predictive models. Overfitting happens when the learning algorithm continues to develop hypotheses that reduce training set error at the cost of an increased test set error. There are several approaches to avoiding overfitting in building decision trees.</p><ul class="postList"><li name="1059" id="1059" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Pre-pruning</strong> that stop growing the tree earlier, before it perfectly classifies the training set.</li><li name="a6c6" id="a6c6" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Post-pruning</strong> that allows the tree to perfectly classify the training set, and then post prune the tree.</li></ul><p name="40e0" id="40e0" class="graf graf--p graf-after--li">Practically, the second approach of post-pruning overfit trees is more successful because it is not easy to precisely estimate when to stop growing the tree. <strong class="markup--strong markup--p-strong">But to solve this overfitting problem in decision we use another technique i.e. Random Forest Classifier.</strong></p><h3 name="1a32" id="1a32" class="graf graf--h3 graf-after--p">Advantages and Disadvantages of Decision Trees in Machine Learning</h3><p name="a4a7" id="a4a7" class="graf graf--p graf-after--h3">Decision Tree is used to solve both classification and regression problems. But the main drawback of Decision Tree is that it generally leads to overfitting of the data. Lets discuss its advantages and disadvantages in detail.</p><p name="b2d4" id="b2d4" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Advantages of Decision Tree</strong></p><p name="eb56" id="eb56" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">1. Clear Visualization</strong>: The algorithm is simple to understand, interpret and visualize as the idea is mostly used in our daily lives. Output of a Decision Tree can be easily interpreted by humans.</p><p name="a971" id="a971" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">2. Simple and easy to understand</strong>: Decision Tree looks like simple <strong class="markup--strong markup--p-strong">if-else statements </strong>which are very easy to understand.</p><p name="059b" id="059b" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">3.</strong> Decision Tree can be used for both <strong class="markup--strong markup--p-strong">classification and regression problems</strong>.</p><p name="65b4" id="65b4" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">4</strong>. Decision Tree can handle both <strong class="markup--strong markup--p-strong">continuous and categorical variables</strong>.</p><p name="2cf3" id="2cf3" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">5.</strong> <strong class="markup--strong markup--p-strong">No feature scaling required</strong>: No feature scaling (standardization and normalization) required in case of Decision Tree as it uses rule based approach instead of distance calculation.</p><p name="4db5" id="4db5" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">6.</strong> Decision Tree can automatically <strong class="markup--strong markup--p-strong">handle missing values</strong>.</p><p name="0dbd" id="0dbd" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">7</strong>. Decision Tree is usually <strong class="markup--strong markup--p-strong">robust to outliers</strong> and can handle them automatically.</p><p name="af24" id="af24" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">9</strong>.<strong class="markup--strong markup--p-strong"> Less Training Period</strong>: Training period is less as compared to Random Forest because it generates only one tree unlike forest of trees in the Random Forest.</p><p name="77a6" id="77a6" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Disadvantages of Decision Tree</strong></p><p name="5fd8" id="5fd8" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">1. Overfitting:</strong> This is the main problem of the Decision Tree. It generally leads to overfitting of the data which ultimately leads to wrong predictions. In order to fit the data (even noisy data), it keeps generating new nodes and ultimately the tree becomes too complex to interpret. In this way, it loses its generalization capabilities. It performs very well on the trained data but starts making a lot of mistakes on the unseen data.</p><p name="c8fc" id="c8fc" class="graf graf--p graf-after--p">I have written a detailed article on Overfitting <a href="https://medium.com/@deeppatel23/ridge-lasso-regression-technique-with-math-for-data-science-part-6-2-ead6246abd8b" data-href="https://medium.com/@deeppatel23/ridge-lasso-regression-technique-with-math-for-data-science-part-6-2-ead6246abd8b" class="markup--anchor markup--p-anchor" target="_blank">here</a>.</p><p name="f7fd" id="f7fd" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">2. High variance: </strong>As mentioned in point 1, Decision Tree generally leads to the overfitting of data. Due to the overfitting, there are very high chances of high variance in the output which leads to many errors in the final estimation and shows high inaccuracy in the results. In order to achieve zero bias (overfitting), it leads to high variance.</p><p name="f768" id="f768" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">3. Unstable: </strong>Adding a new data point can lead to re-generation of the overall tree and all nodes need to be recalculated and recreated.</p><p name="3c77" id="3c77" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">4. Affected by noise:</strong> Little bit of noise can make it unstable which leads to wrong predictions.</p><p name="dc0d" id="dc0d" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">5. Not suitable for large datasets:</strong> If data size is large, then one single tree may grow complex and lead to overfitting. So in this case, we should use Random Forest instead of a single Decision Tree.</p><p name="d464" id="d464" class="graf graf--p graf-after--p">In order to overcome the limitations of the Decision Tree, we should use Random Forest which does not rely on a single tree. It creates a forest of trees and takes the decision based on the vote count. Random Forest is based on bagging method which is one of the Ensemble Learning techniques.</p><p name="5dfa" id="5dfa" class="graf graf--p graf-after--p">My next blog will be on <strong class="markup--strong markup--p-strong">Random Forest</strong>. I hope you enjoyed this blog. Make sure that you <strong class="markup--strong markup--p-strong">follow</strong> me for more blogs related to this topic. Have a Great Day!!!</p><figure name="8c7d" id="8c7d" class="graf graf--figure graf-after--p graf--trailing"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 283px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 40.400000000000006%;"></div><img class="graf-image" data-image-id="0*irzsjkHrfp6y6gQg" data-width="952" data-height="385" src="https://cdn-images-1.medium.com/max/800/0*irzsjkHrfp6y6gQg"></div><figcaption class="imageCaption">Thanks for reading my blog.</figcaption></figure></div></div></section>
</section><!--
<footer><p>By <a href="https://medium.com/@deeppatel23" class="p-author h-card">Deep Patel</a> on <a href="https://medium.com/p/4062186b8a9b"><time class="dt-published" datetime="2020-06-28T09:50:18.125Z">June 28, 2020</time></a>.</p><p><a href="https://medium.com/@deeppatel23/decision-tree-classifier-data-science-ml-part-7-2-4062186b8a9b" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on September 4, 2020.</p></footer></article>--></body></html>