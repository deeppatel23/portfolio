<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Logistic Regression Classification | Math Intuition</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Logistic Regression Classification | Math Intuition</h1>
</header>
<section data-field="subtitle" class="p-summary">
Why Logistic Regression being a classification technique, still name regression.
</section>
<section data-field="body" class="e-content">
<section name="36a9" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="ca59" id="ca59" class="graf graf--h3 graf--leading graf--title">Logistic Regression Classification | Math | Data Science | ML(Part 7.1 )</h3><figure name="db27" id="db27" class="graf graf--figure graf--layoutOutsetLeft graf-after--h3"><div class="aspectRatioPlaceholder is-locked" style="max-width: 525px; max-height: 245px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 46.6%;"></div><img class="graf-image" data-image-id="0*Bf8wkUJoV0Tx0T8y.PNG" data-width="972" data-height="453" data-is-featured="true" src="https://cdn-images-1.medium.com/max/600/0*Bf8wkUJoV0Tx0T8y.PNG"></div><figcaption class="imageCaption">Email Spam CLASSIFIER</figcaption></figure><p name="fb04" id="fb04" class="graf graf--p graf-after--figure">Hello Guys!!!</p><p name="e967" id="e967" class="graf graf--p graf-after--p">So far, we have discussed about <a href="https://medium.com/@deeppatel23/regression-data-science-part-6-linear-regression-with-math-6-1-e75dbae11937" data-href="https://medium.com/@deeppatel23/regression-data-science-part-6-linear-regression-with-math-6-1-e75dbae11937" class="markup--anchor markup--p-anchor" target="_blank">Regression</a> technique. Now, lets look at second main <a href="https://medium.com/@deeppatel23/types-processes-of-machine-learning-data-science-part-4-b0d287dc0068" data-href="https://medium.com/@deeppatel23/types-processes-of-machine-learning-data-science-part-4-b0d287dc0068" class="markup--anchor markup--p-anchor" target="_blank">Supervised</a> learning algorithm, i.e. Classification.</p><p name="3ff1" id="3ff1" class="graf graf--p graf--hasDropCapModel graf--hasDropCap graf-after--p"><span class="graf-dropCap">C</span><strong class="markup--strong markup--p-strong">an classification problems be solved using Linear Regression?</strong></p><p name="81e8" id="81e8" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Why Logistic Regression being a classification technique, still named regression? </strong>All these questions will be answered in this blog.</p><h3 name="bc09" id="bc09" class="graf graf--h3 graf-after--p"><strong class="markup--strong markup--h3-strong">Introduction to Classification</strong></h3><p name="caac" id="caac" class="graf graf--p graf-after--h3">Classification is a subcategory of supervised learning, where the process is of categorizing a given set of data into classes. Classes such as <em class="markup--em markup--p-em">Good</em><strong class="markup--strong markup--p-strong">/</strong><em class="markup--em markup--p-em">Bad</em>(<strong class="markup--strong markup--p-strong">reviews</strong>), <em class="markup--em markup--p-em">Spam</em><strong class="markup--strong markup--p-strong">/</strong><em class="markup--em markup--p-em">Not spam</em>(<strong class="markup--strong markup--p-strong">Emails</strong>)<em class="markup--em markup--p-em"> or Positive</em><strong class="markup--strong markup--p-strong">/</strong><em class="markup--em markup--p-em">Negative</em><strong class="markup--strong markup--p-strong">/</strong><em class="markup--em markup--p-em">Neutral </em>(<strong class="markup--strong markup--p-strong">tweets</strong>). It can be performed on both structured or unstructured data. The process starts with predicting the class of given data points. The classes are often referred to as target, label or categories.</p><p name="1dbc" id="1dbc" class="graf graf--p graf-after--p">The classification predictive modeling is the task of approximating the mapping function from input variables to discrete output variables. The main goal is to identify which class/category the new data will fall into.</p><p name="a0f1" id="a0f1" class="graf graf--p graf-after--p">There are two main types of classification problems:</p><ul class="postList"><li name="b985" id="b985" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Binary</strong> classification: The typical example is e-mail spam detection, which each e-mail is spam → 1 spam; or isn’t → 0.</li><li name="8b33" id="8b33" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Multi-class</strong> classification: Like handwritten character recognition (where classes go from 0 to 9).</li></ul><blockquote name="3045" id="3045" class="graf graf--blockquote graf-after--li">Let’s try to understand this with an example.</blockquote><p name="c5c1" id="c5c1" class="graf graf--p graf-after--blockquote">Whenever, we get an email, the spam classifier classifies it. The classifier uses Natural Language Processing(will be covered in future blogs) with ML to process the received text in the mail. It than makes a decision on the basis of types of words or sentences used in the mail. Then it classifies, whether that mail is spam or not.</p><p name="9990" id="9990" class="graf graf--p graf-after--p">Real-life <strong class="markup--strong markup--p-strong">application</strong> of Classification:</p><ul class="postList"><li name="4646" id="4646" class="graf graf--li graf-after--p">Speech Recognition</li><li name="5fd5" id="5fd5" class="graf graf--li graf-after--li">Spam classification</li><li name="57c1" id="57c1" class="graf graf--li graf-after--li">Face detection</li><li name="6641" id="6641" class="graf graf--li graf-after--li">Handwriting recognition</li></ul><h3 name="8ef9" id="8ef9" class="graf graf--h3 graf-after--li"><strong class="markup--strong markup--h3-strong">Types Of Learners In Classification</strong></h3><ul class="postList"><li name="9562" id="9562" class="graf graf--li graf-after--h3"><strong class="markup--strong markup--li-strong">Lazy Learners</strong> — Lazy learners simply store the training data and wait until a testing data appears. The classification is done using the most related data in the stored training data. They have more predicting time compared to eager learners. Eg — k-nearest neighbor, case-based reasoning.</li><li name="f7b2" id="f7b2" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Eager Learners</strong> — Eager learners construct a classification model based on the given training data before getting data for predictions. It must be able to commit to a single hypothesis that will work for the entire space. Due to this, they take a lot of time in training and less time for a prediction. Eg — Decision Tree, Naïve Bayes, Artificial Neural Networks.</li></ul><p name="26f4" id="26f4" class="graf graf--p graf-after--li">Now, we know the basics of classification. But before we start LogisticRegression, I would like to answer, <strong class="markup--strong markup--p-strong">Why can’t we use linear regression for classification?</strong></p><blockquote name="ec0a" id="ec0a" class="graf graf--blockquote graf-after--p">To understand this, we will look at the following example:</blockquote><p name="0772" id="0772" class="graf graf--p graf-after--blockquote">Let’s say we create a perfectly balanced dataset, where it contains a list of customers and a label to determine if the customer had purchased. In the dataset, there are 20 customers; 10 customers age between 10 to 19 who purchased, and 10 customers age between 20 to 29 who did not purchase. “Purchased” is a binary label denote by 0 and 1, where 0 denote “customer did not make a purchase” and 1 denote “customer made a purchase”.</p><figure name="b5ea" id="b5ea" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 363px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 51.9%;"></div><img class="graf-image" data-image-id="1*EI75w7NlZLbJF4JrS-g2EQ.png" data-width="794" data-height="412" src="https://cdn-images-1.medium.com/max/800/1*EI75w7NlZLbJF4JrS-g2EQ.png"></div><figcaption class="imageCaption">Data Point on Graph</figcaption></figure><p name="b1cb" id="b1cb" class="graf graf--p graf-after--figure">The objective of a linear regression model is to find a relationship between the input variables and a target variable. Below is our linear regression model that was trained using the above dataset. The red line is the best fit line for the training dataset, which aims to minimize the distance between the predicted value and actual value.</p><figure name="2be5" id="2be5" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 355px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 50.8%;"></div><img class="graf-image" data-image-id="1*C1fSExY2CSryh-_X421xtg.png" data-width="796" data-height="404" src="https://cdn-images-1.medium.com/max/800/1*C1fSExY2CSryh-_X421xtg.png"></div><figcaption class="imageCaption">Linear Regression PLOT</figcaption></figure><p name="a3a9" id="a3a9" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">First problem with Linear Regression</strong>: It outputs continuous answers, whereas we need just two answers ‘0’ or ‘1’. Since it&#39;s just binary classification, we can solve this problem by stating that — if Y is greater than 0.5 (above the green line), predict that this customer will make purchases otherwise will not make purchases. But, this <strong class="markup--strong markup--p-strong">won&#39;t</strong> work in multi-class classifier.</p><p name="6f2a" id="6f2a" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Second problem “The effect of outliers”</strong>:</p><p name="48f1" id="48f1" class="graf graf--p graf-after--p">Let’s add 10 more customers age between 60 to 70, and train our linear regression model, finding the best fit line.</p><figure name="f8cc" id="f8cc" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 358px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 51.2%;"></div><img class="graf-image" data-image-id="1*fVQx2nZLXuxzxdoatE05NQ.png" data-width="789" data-height="404" src="https://cdn-images-1.medium.com/max/800/1*fVQx2nZLXuxzxdoatE05NQ.png"></div><figcaption class="imageCaption">Relation Line changed by adding more DATA</figcaption></figure><p name="d51b" id="d51b" class="graf graf--p graf-after--figure">Our linear regression model manages to fit a new line, but if you look closer, some customers (age 20 to 22) outcome are predicted <strong class="markup--strong markup--p-strong">wrongly</strong>. As linear regression tries to fit the regression line by minimizing prediction error, in order to minimize the distance of predicted and actual value for customers age between 60 to 70. This can only be solved by <strong class="markup--strong markup--p-strong">Logistic Regression.</strong></p><h3 name="60c3" id="60c3" class="graf graf--h3 graf-after--p">Logistic Regression</h3><figure name="7629" id="7629" class="graf graf--figure graf-after--h3"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 366px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 52.300000000000004%;"></div><img class="graf-image" data-image-id="1*yzOO2R_zQq1ubT8imXZc1A.png" data-width="792" data-height="414" src="https://cdn-images-1.medium.com/max/800/1*yzOO2R_zQq1ubT8imXZc1A.png"></div><figcaption class="imageCaption"><strong class="markup--strong markup--figure-strong">L</strong>ogistic <strong class="markup--strong markup--figure-strong">R</strong>egression on <strong class="markup--strong markup--figure-strong">above</strong> problem</figcaption></figure><p name="fd9c" id="fd9c" class="graf graf--p graf-after--figure">Logistic regression is a classification algorithm used to assign observations to a discrete set of classes. Unlike linear regression which outputs continuous number values, logistic regression transforms its output using the logistic <strong class="markup--strong markup--p-strong">sigmoid</strong> function to return a probability value which can then be mapped to two or more discrete classes.</p><h3 name="a7fb" id="a7fb" class="graf graf--h3 graf-after--p">Binary Logistic Regression</h3><p name="d25d" id="d25d" class="graf graf--p graf-after--h3">Here we predict two values like- yes/no. Our previous example, whether the customer will purchase the product or not, is an example of binary logistic regression.</p><blockquote name="167b" id="167b" class="graf graf--blockquote graf-after--p">Making Prediction:</blockquote><p name="fe58" id="fe58" class="graf graf--p graf-after--blockquote">Let’s use the same multiple linear regression equation from our <a href="https://medium.com/@deeppatel23/regression-data-science-part-6-linear-regression-with-math-6-1-e75dbae11937" data-href="https://medium.com/@deeppatel23/regression-data-science-part-6-linear-regression-with-math-6-1-e75dbae11937" class="markup--anchor markup--p-anchor" target="_blank">linear regression tutorial</a>.</p><figure name="1aaa" id="1aaa" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 267px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 38.1%;"></div><img class="graf-image" data-image-id="1*i_CAX3FD69tCchQ5i7HF7A.png" data-width="1280" data-height="488" src="https://cdn-images-1.medium.com/max/800/1*i_CAX3FD69tCchQ5i7HF7A.png"></div><figcaption class="imageCaption"><strong class="markup--strong markup--figure-strong">P</strong> is applied <strong class="markup--strong markup--figure-strong">sigmoid</strong> function</figcaption></figure><blockquote name="0ebc" id="0ebc" class="graf graf--blockquote graf-after--figure">Cost Function</blockquote><p name="fb84" id="fb84" class="graf graf--p graf-after--blockquote">We wont use use the same cost function MSE as we did for linear regression. Because here the prediction function is non-linear due to sigmoid transformation. Thus, here MSE will results in many local minima and gradient descent may not find the optimal global minimum.</p><p name="793f" id="793f" class="graf graf--p graf-after--p">Instead of Mean Squared Error, we use a cost function called Cross Entropy, also known as Log Loss. Cross-entropy loss can be divided into two separate cost functions: one for y=1 and one for y=0.</p><figure name="6d54" id="6d54" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 363px; max-height: 111px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 30.599999999999998%;"></div><img class="graf-image" data-image-id="0*oeN8Ivzacr6CpEV2.png" data-width="363" data-height="111" src="https://cdn-images-1.medium.com/max/800/0*oeN8Ivzacr6CpEV2.png"></div><figcaption class="imageCaption"><strong class="markup--strong markup--figure-strong">COST Function</strong></figcaption></figure><figure name="d527" id="d527" class="graf graf--figure graf--layoutOutsetLeft graf-after--figure"><div class="aspectRatioPlaceholder is-locked" style="max-width: 525px; max-height: 423px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 80.60000000000001%;"></div><img class="graf-image" data-image-id="0*qusC5NRJBBHyyH_o.png" data-width="928" data-height="748" src="https://cdn-images-1.medium.com/max/600/0*qusC5NRJBBHyyH_o.png"></div></figure><p name="42d8" id="42d8" class="graf graf--p graf-after--figure">The benefits of taking the logarithm reveal themselves when you look at the cost function graphs for y=1 and y=0. These smooth <strong class="markup--strong markup--p-strong">monotonic</strong> functions (always increasing or always decreasing) make it easy to calculate the gradient and minimize cost.</p><p name="1ffb" id="1ffb" class="graf graf--p graf-after--p">The key thing to note is the cost function penalizes confident and wrong predictions more than it rewards confident and right predictions!</p><p name="c31c" id="c31c" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Above functions compressed into one</strong></p><figure name="15fb" id="15fb" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 443px; max-height: 56px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 12.6%;"></div><img class="graf-image" data-image-id="0*qLKXKAkzYGLN2Duj.png" data-width="443" data-height="56" src="https://cdn-images-1.medium.com/max/800/0*qLKXKAkzYGLN2Duj.png"></div><figcaption class="imageCaption"><strong class="markup--strong markup--figure-strong">C</strong>ompressed <strong class="markup--strong markup--figure-strong">COST </strong>function</figcaption></figure><h3 name="2e26" id="2e26" class="graf graf--h3 graf-after--figure">Gradient descent</h3><p name="f59f" id="f59f" class="graf graf--p graf-after--h3">To minimize our cost, we use Gradient Descent just like before in Linear Regression. There are other more sophisticated optimization algorithms out there such as conjugate gradient like BFGS<code class="markup--code markup--p-code">(batch gradient descent)</code>, but you don’t have to worry about these. Machine learning libraries like Scikit-learn hide their implementations so you can focus on more interesting things!</p><h3 name="411d" id="411d" class="graf graf--h3 graf-after--p">Due to this underlying technique is quite the same as <strong class="markup--strong markup--h3-strong">Linear “Regression”</strong>, this algorithm is also named as Logistic “<strong class="markup--strong markup--h3-strong">Regression”</strong>.</h3><h3 name="63b0" id="63b0" class="graf graf--h3 graf-after--h3">Multiclass logistic regression</h3><p name="b2c3" id="b2c3" class="graf graf--p graf-after--h3">Instead of y=0,1 we will expand our definition so that y=0,1…n. Basically we re-run binary classification multiple times, once for each class.</p><p name="6a1c" id="6a1c" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Procedure</strong></p><ul class="postList"><li name="966f" id="966f" class="graf graf--li graf-after--p">Divide the problem into n+1 binary classification problems (+1 because the index starts at 0?).</li><li name="9ee7" id="9ee7" class="graf graf--li graf-after--li">Predict the probability the observations are in that single class.</li><li name="380f" id="380f" class="graf graf--li graf-after--li">prediction = max(probability of the classes)</li></ul><p name="97dc" id="97dc" class="graf graf--p graf-after--li">For each sub-problem, we select one class (YES) and lump all the others into a second class (NO). Then we take the class with the highest predicted value. Here, we use SOFTMAX function to do this.</p><p name="051f" id="051f" class="graf graf--p graf-after--p">The <strong class="markup--strong markup--p-strong">SoftMax regression</strong> is a form of logistic regression that normalizes an input value into a vector of values that follows a probability distribution whose total sums up to 1. The output values are between the range [0,1] which is nice because we are able to avoid binary classification and accommodate as many classes or dimensions in our neural network model. <em class="markup--em markup--p-em">This function involves use of artificial neural network.</em> This is why SoftMax is sometimes referred to as a multinomial logistic regression. The standard (unit) SoftMax function is defined by the formula:</p><figure name="6b91" id="6b91" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 99px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 14.099999999999998%;"></div><img class="graf-image" data-image-id="1*v-I8kDUPRipmYDqXJa5A3w.png" data-width="737" data-height="104" src="https://cdn-images-1.medium.com/max/800/1*v-I8kDUPRipmYDqXJa5A3w.png"></div><figcaption class="imageCaption">SoftMax Intuition</figcaption></figure><p name="fa5f" id="fa5f" class="graf graf--p graf-after--figure">In words: we apply the standard exponential function to each element zi of the input vector z and normalize these values by dividing by the sum of all these exponentials; this normalization ensures that the sum of the components of the output vector σ(z) is 1.</p><p name="b00d" id="b00d" class="graf graf--p graf-after--p">There is a great depth in this function. Since we have scikit-learn Machine Learning library to do this in simple steps.</p><pre name="999c" id="999c" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">#same code for all types of classifier<br>from</strong> sklearn.linear_model <strong class="markup--strong markup--pre-strong">import</strong> LogisticRegression</pre><pre name="385b" id="385b" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">#making a model</strong> <br>logreg = LogisticRegression()</pre><pre name="0a87" id="0a87" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">#training our model</strong><br>logreg.fit(x_train, y_train)</pre><pre name="3712" id="3712" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">#predict output of model</strong><br>test_y_hat = logreg.predict(x_test)</pre><pre name="c519" id="c519" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">#test the model</strong><br>print(&quot;Mean absolute error: %.2f&quot; % np.mean(np.absolute(test_y_hat - y_test)))</pre><pre name="d327" id="d327" class="graf graf--pre graf-after--pre">print(&quot;Residual sum of squares (MSE): %.2f&quot; % np.mean((test_y_hat - y_test) ** 2))</pre><pre name="bf56" id="bf56" class="graf graf--pre graf-after--pre">from sklearn.metrics import accuracy_score<br>accuracy_score(y_test, test_y_hat)</pre><p name="f0a2" id="f0a2" class="graf graf--p graf-after--pre">A <strong class="markup--strong markup--p-strong">confusion matrix</strong> is a table that is often used to describe the <em class="markup--em markup--p-em">performance of a classification model</em> (or “classifier”) on a set of test data for which the true values are known. The <strong class="markup--strong markup--p-strong">confusion matrix</strong> itself is relatively simple to understand, but the related terminology can be <strong class="markup--strong markup--p-strong">confusing</strong>. In simple language, when same types of values coincide i.e. TP &amp; TN are the correctly predicted values.</p><figure name="d823" id="d823" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 356px; max-height: 267px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 75%;"></div><img class="graf-image" data-image-id="0*PP-ga9APSiTx0Nr8.png" data-width="356" data-height="267" src="https://cdn-images-1.medium.com/max/800/0*PP-ga9APSiTx0Nr8.png"></div><figcaption class="imageCaption">Confusion Matrix</figcaption></figure><pre name="09fc" id="09fc" class="graf graf--pre graf-after--figure"><strong class="markup--strong markup--pre-strong">#plotting confusion metrics </strong><br>from sklearn import metrics<br>cnf_matrix = metrics.confusion_matrix(y_test, test_y_hat)<br>sns.heatmap(cnf_matrix, annot=True)<br>plt.title(&quot;Confusion Matrix&quot;)<br>plt.show()</pre><p name="bb03" id="bb03" class="graf graf--p graf-after--pre graf--trailing">This leads to the end of this blog. Make sure you follow me to read each and every blog. I hope you liked it and understood most of it. Good BYE, Have a great Day.</p></div></div></section>
</section><!--
<footer><p>By <a href="https://medium.com/@deeppatel23" class="p-author h-card">Deep Patel</a> on <a href="https://medium.com/p/a412b0358137"><time class="dt-published" datetime="2020-06-27T22:31:00.975Z">June 27, 2020</time></a>.</p><p><a href="https://medium.com/@deeppatel23/classification-technique-math-data-science-ml-logistic-regression-part-7-1-a412b0358137" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on September 4, 2020.</p></footer>--></article></body></html>