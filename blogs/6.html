<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Captain Regression</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Captain Regression</h1>
</header>
<section data-field="subtitle" class="p-summary">
Linear regression, the first AVENGER in ML Algorithm.
</section>
<section data-field="body" class="e-content">
<section name="0865" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="7b54" id="7b54" class="graf graf--h3 graf--leading graf--title">Regression(Data Science Part 6) Linear Regression with Math (6.1)</h3><p name="e6e0" id="e6e0" class="graf graf--p graf-after--h3">Linear regression, the first AVENGER in ML Algorithm.</p><figure name="28ac" id="28ac" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 394px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 56.3%;"></div><img class="graf-image" data-image-id="1*nNS5XWi0qS8ETthaL0BnZw.jpeg" data-width="3840" data-height="2160" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/1*nNS5XWi0qS8ETthaL0BnZw.jpeg"></div><figcaption class="imageCaption">YAAH!! Linear Regression is really First Avenger Algorithm (Source: Author)</figcaption></figure><figure name="93dc" id="93dc" class="graf graf--figure graf--layoutOutsetLeft graf-after--figure"><div class="aspectRatioPlaceholder is-locked" style="max-width: 525px; max-height: 347px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 66.2%;"></div><img class="graf-image" data-image-id="0*cngZQlrDQ1F-SFIf.png" data-width="680" data-height="450" src="https://cdn-images-1.medium.com/max/600/0*cngZQlrDQ1F-SFIf.png"></div><figcaption class="imageCaption">Source: Author</figcaption></figure><blockquote name="a94e" id="a94e" class="graf graf--pullquote graf-after--figure">When we talk about regression, the first thing that comes to our mind is a scatter plot image, somewhat similar to this. Now, we will understand all parts and types of regression in detail.</blockquote><p name="1fd8" id="1fd8" class="graf graf--p graf-after--pullquote">To establish the possible <strong class="markup--strong markup--p-strong">relationship </strong>among different variables, various modes of statistical approaches are implemented, known as regression analysis. In order to understand how the variation in an independent variable can impact the dependent variable, regression analysis is specially molded out.</p><p name="d05c" id="d05c" class="graf graf--p graf-after--p">Basically, regression analysis sets up an equation to explain the significant relationship between one or more predictors and response variables and also to estimate current observations. The regression outcomes lead to the identification of the direction, size, and analytical significance of the relationship between predictor and response where the dependent variable could be numerical or discrete in nature.</p><figure name="6b8d" id="6b8d" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 427px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 60.9%;"></div><img class="graf-image" data-image-id="0*qq-5uT4Tk_w7WyEt.jpg" data-width="832" data-height="507" src="https://cdn-images-1.medium.com/max/800/0*qq-5uT4Tk_w7WyEt.jpg"></div><figcaption class="imageCaption">Just a <strong class="markup--strong markup--figure-strong">JOKE</strong></figcaption></figure><h3 name="5ab1" id="5ab1" class="graf graf--h3 graf-after--figure">WHERE do we use regression.</h3><ol class="postList"><li name="d494" id="d494" class="graf graf--li graf-after--h3">Hours spent studying Vs Marks scored by students</li><li name="2c50" id="2c50" class="graf graf--li graf-after--li">Amount of rainfall Vs Agricultural yield</li><li name="f5b0" id="f5b0" class="graf graf--li graf-after--li">Electricity usage Vs Electricity bill</li><li name="3d5e" id="3d5e" class="graf graf--li graf-after--li">Suicide rates Vs Number of stressful people</li><li name="3ea6" id="3ea6" class="graf graf--li graf-after--li">Years of experience Vs Salary</li><li name="a54c" id="a54c" class="graf graf--li graf-after--li">Demand Vs Product price</li><li name="b41c" id="b41c" class="graf graf--li graf-after--li">Age Vs Beauty</li><li name="12bc" id="12bc" class="graf graf--li graf-after--li">Age Vs Health issues</li><li name="3aa2" id="3aa2" class="graf graf--li graf-after--li">Number of Degrees Vs Salary</li><li name="0362" id="0362" class="graf graf--li graf-after--li">Number of Degrees Vs Education expenditure</li></ol><h3 name="f59d" id="f59d" class="graf graf--h3 graf-after--li">Types of regression techniques</h3><p name="75de" id="75de" class="graf graf--p graf-after--h3">In addition to it, the types of regression analysis can be selected on the attributes, target variables, or the shape and nature of the regression curve that exhibit the relationship between dependent and independent variables. In this blog, we will discuss <strong class="markup--strong markup--p-strong">linear regression</strong> with MATH in Detail.</p><h3 name="cf04" id="cf04" class="graf graf--h3 graf-after--p">Introduction</h3><p name="dd86" id="dd86" class="graf graf--p graf-after--h3">Linear Regression is a supervised machine learning algorithm where the predicted output is continuous and has a constant slope. It’s used to predict values within a continuous range, (e.g. sales, price) rather than trying to classify them into categories (e.g. cat, dog). There are two main types:</p><p name="3bb6" id="3bb6" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Simple regression</strong></p><p name="152b" id="152b" class="graf graf--p graf-after--p">Simple linear regression uses traditional slope-intercept form, where m and b are the variables. Our algorithm will try to “learn” to produce the most accurate predictions where <strong class="markup--strong markup--p-strong">x</strong> represents our input data and <strong class="markup--strong markup--p-strong">y</strong> represents our prediction.</p><pre name="3c94" id="3c94" class="graf graf--pre graf-after--p">y=mx+b ; m: slope, b: intercept</pre><p name="c96c" id="c96c" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">Multivariable regression</strong></p><p name="75a2" id="75a2" class="graf graf--p graf-after--p">A more complex, multi-variable linear equation might look like this, where w represents the coefficients, or weights, our model will try to learn.</p><pre name="54e5" id="54e5" class="graf graf--pre graf-after--p">f(x,y,z)=w1x+w2y+w3z : x, y, z are three input parameters</pre><p name="9e0f" id="9e0f" class="graf graf--p graf-after--pre">The variables x, y, z represent the attributes, or distinct pieces of information, we have about each observation. For sales predictions, these attributes might include a company’s advertising spend on radio, TV, and newspapers.</p><pre name="a79f" id="a79f" class="graf graf--pre graf-after--p">Sales=w1Radio+w2TV+w3News</pre><h3 name="0903" id="0903" class="graf graf--h3 graf-after--pre">Simple Regression</h3><p name="db3d" id="db3d" class="graf graf--p graf-after--h3">Let’s say we are given a dataset with the following columns (features): how much a company spends on Radio advertising each year and its annual Sales in terms of units sold. We are trying to develop an equation that will let us to predict units sold based on how much a company spends on radio advertising. The rows (observations) represent companies.</p><figure name="bfae" id="bfae" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 421px; max-height: 269px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 63.9%;"></div><img class="graf-image" data-image-id="1*6mnFw2wINsNiSgVQFvJTWw.png" data-width="421" data-height="269" src="https://cdn-images-1.medium.com/max/800/1*6mnFw2wINsNiSgVQFvJTWw.png"></div><figcaption class="imageCaption">Data Frame</figcaption></figure><h3 name="206a" id="206a" class="graf graf--h3 graf-after--figure">Making Prediction</h3><p name="1aad" id="1aad" class="graf graf--p graf-after--h3">Our prediction function outputs an estimate of sales given a company’s radio advertising spend and our current values for <em class="markup--em markup--p-em">Weight</em> and <em class="markup--em markup--p-em">Bias</em>.</p><pre name="7613" id="7613" class="graf graf--pre graf-after--p">Sales=Weight⋅Radio+Bias</pre><p name="d2be" id="d2be" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">Weight</strong>: the coefficient for the Radio independent variable. In machine learning we call coefficients <em class="markup--em markup--p-em">weights</em>.</p><p name="064e" id="064e" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Radio</strong>: the independent variable. In machine learning we call these variables <em class="markup--em markup--p-em">features</em>.</p><p name="79fc" id="79fc" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Bias</strong>: the intercept where our line intercepts the y-axis. In machine learning we can call intercepts <em class="markup--em markup--p-em">bias</em>. Bias offsets all predictions that we make.</p><blockquote name="f84d" id="f84d" class="graf graf--pullquote graf-after--p">Our algorithm will try to <em class="markup--em markup--pullquote-em">learn</em> the correct values for Weight and Bias. By the end of our training, our equation will approximate the <em class="markup--em markup--pullquote-em">line of best fit</em>. For updating these weight and biases, we will introduce a cost function (or <strong class="markup--strong markup--pullquote-strong">loss function</strong>) and try to reduce it’s value.</blockquote><h3 name="4822" id="4822" class="graf graf--h3 graf-after--pullquote"><strong class="markup--strong markup--h3-strong">What is Cost Function?</strong></h3><figure name="809a" id="809a" class="graf graf--figure graf-after--h3"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 287px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 40.9%;"></div><img class="graf-image" data-image-id="0*0C2vtf5RT6hNyIUo.png" data-width="1400" data-height="573" src="https://cdn-images-1.medium.com/max/800/0*0C2vtf5RT6hNyIUo.png"></div><figcaption class="imageCaption"><strong class="markup--strong markup--figure-strong">COST FUNCTION </strong>(Source: <a href="https://ml-cheatsheet.readthedocs.io/en/latest/" data-href="https://ml-cheatsheet.readthedocs.io/en/latest/" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">here</a>)</figcaption></figure><p name="f463" id="f463" class="graf graf--p graf-after--figure">The primary set-up for updating weight and biases is to define a cost function (also known as a loss function) that measures how well the network predicts outputs on the test set. <strong class="markup--strong markup--p-strong">The goal is to then find a set of weights and biases that minimizes the cost.</strong> One common function that is often used is the <strong class="markup--strong markup--p-strong">mean squared error</strong>, which measures the difference between the actual value of y and the estimated value of y (the prediction). The equation of the below regression line is hθ(x) = θ + θ1x, which has only two parameters: weight (θ1)and bias (θ0). <em class="markup--em markup--p-em">Observe above image carefully and you will understand errors, that need to be corrected by calculating cost function and reducing it’s value.</em></p><p name="8af0" id="8af0" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Math</strong></p><p name="b131" id="b131" class="graf graf--p graf-after--p">Given our simple linear equation y=mx+b we can calculate <strong class="markup--strong markup--p-strong">MSE</strong> as:</p><pre name="c9b2" id="c9b2" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">MSE=1N∑i=1n(yi−(mxi+b))2</strong><br>N is the total number of observations (data points)<br>1N∑i=1n is the mean<br>yi is the actual value of an observation and mxi+b is our prediction</pre><p name="f105" id="f105" class="graf graf--p graf-after--pre">IS Variance and MSE same?</p><p name="dd72" id="dd72" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Variance</strong> is the measure of how far the data points are spread out whereas, <strong class="markup--strong markup--p-strong">MSE (Mean Squared Error)</strong> is the measure of how actually the predicted values are different from the actual values. Though, both are the measures of second moment but there is a significant difference. <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">In general, the sample variance measures the spread of the data around the mean (in squared units), while the MSE measures the vertical spread of the data around the regression line (in squared vertical units). </em></strong>Hope you don’t get confused by these terms.</p><h3 name="9bd1" id="9bd1" class="graf graf--h3 graf-after--p">Gradient Descent</h3><figure name="9856" id="9856" class="graf graf--figure graf-after--h3"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 312px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 44.5%;"></div><img class="graf-image" data-image-id="0*Lq_heBNiOuyRkRuz.png" data-width="1400" data-height="623" src="https://cdn-images-1.medium.com/max/800/0*Lq_heBNiOuyRkRuz.png"></div><figcaption class="imageCaption"><strong class="markup--strong markup--figure-strong">GRADIENT DESCENT </strong>(Source: <a href="https://ml-cheatsheet.readthedocs.io/en/latest/" data-href="https://ml-cheatsheet.readthedocs.io/en/latest/" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">here</a>)</figcaption></figure><p name="c8ad" id="c8ad" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">To minimize MSE</strong>, we use Gradient Descent to calculate the gradient of our cost function. Gradient Descent runs iteratively to find the optimal values of the parameters corresponding to the <strong class="markup--strong markup--p-strong">minimizing value of the given cost function, using calculus.</strong> Mathematically, the technique of the ‘derivative’ is extremely important to minimize the cost function because it helps get the minimum point. The derivative is a concept from calculus and refers to the slope of the function at a given point. We need to know the slope so that we know the direction (sign) to move the coefficient values in order to get a lower cost on the next iteration.</p><p name="fb9f" id="fb9f" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Math</strong></p><p name="32b0" id="32b0" class="graf graf--p graf-after--p">There are two parameters (coefficients) in our cost function we can control: weight mm and bias bb. Since we need to consider the impact each one has on the final prediction, we use partial derivatives. To find the partial derivatives, we use the <strong class="markup--strong markup--p-strong">Chain Rule</strong>. We need the chain rule because (y−(mx+b))^2 is really 2 nested functions: the inner function y−(mx+b) and the outer function x^2.</p><p name="6a9a" id="6a9a" class="graf graf--p graf-after--p">Returning to our cost function:</p><figure name="d55e" id="d55e" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 428px; max-height: 105px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 24.5%;"></div><img class="graf-image" data-image-id="1*TicG-vbY6M_2yNKS-P6IfA.png" data-width="428" data-height="105" src="https://cdn-images-1.medium.com/max/800/1*TicG-vbY6M_2yNKS-P6IfA.png"></div><figcaption class="imageCaption">COST FUNCTION</figcaption></figure><p name="e384" id="e384" class="graf graf--p graf-after--figure">First we will look at Chain Rule:</p><figure name="f18d" id="f18d" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 382px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 54.50000000000001%;"></div><img class="graf-image" data-image-id="1*JEGSLRchXxT5XMfO5iJwyg.png" data-width="853" data-height="465" src="https://cdn-images-1.medium.com/max/800/1*JEGSLRchXxT5XMfO5iJwyg.png"></div><figcaption class="imageCaption"><strong class="markup--strong markup--figure-strong">CHAIN RULE</strong></figcaption></figure><p name="f517" id="f517" class="graf graf--p graf-after--figure">We can calculate the <strong class="markup--strong markup--p-strong">gradient</strong> of this cost function as:</p><figure name="ed59" id="ed59" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 674px; max-height: 260px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 38.6%;"></div><img class="graf-image" data-image-id="1*3MSb5qS0eKLO5YB5nQXPTQ.png" data-width="674" data-height="260" src="https://cdn-images-1.medium.com/max/800/1*3MSb5qS0eKLO5YB5nQXPTQ.png"></div><figcaption class="imageCaption">Applying Gradient Descent to Cost function</figcaption></figure><blockquote name="3e77" id="3e77" class="graf graf--pullquote graf-after--figure">BUT, THIS IS TOO MUCH CODE. WE DONT CODE THESE DERIVATIVES SEPARATELY. WE DIRECTLY USE LINEAR REGRESSION MODULE FROM SCIKIT-LEARN LIBRARY THAT AUTOMATICALLY CALCULATE ALL WEIGHTS &amp; BIASES &amp; COMPUTE THE RESULTS.</blockquote><pre name="19c7" id="19c7" class="graf graf--pre graf-after--pullquote"><strong class="markup--strong markup--pre-strong">#First splitting data for training data(85%) and testing data(15%)<br>#test_size can be changed <br>#random_state Controls the shuffling applied to the data before applying the split 1==True<br>#Here x_data = df[&#39;Radio ($)&#39;] &amp; y_data = df[&#39;Sales&#39;]</strong></pre><pre name="3ee8" id="3ee8" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">from</strong> sklearn.model_selection <strong class="markup--strong markup--pre-strong">import</strong> train_test_split<br>x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.15, random_state=1)</pre><pre name="5869" id="5869" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">#Creating linear regression model<br>from</strong> sklearn.linear_model <strong class="markup--strong markup--pre-strong">import</strong> LinearRegression<br>#Create the linear regression object<br>lm = LinearRegression()</pre><pre name="7152" id="7152" class="graf graf--pre graf-after--pre">lm.fit(x_train, y_train)<br>test_y_hat = lm.predict(x_test)</pre><pre name="9d47" id="9d47" class="graf graf--pre graf-after--pre">print(&quot;<strong class="markup--strong markup--pre-strong">Mean absolute error:</strong> %.2f&quot; % np.mean(np.absolute(test_y_hat - y_test)))</pre><pre name="cea0" id="cea0" class="graf graf--pre graf-after--pre">print(&quot;<strong class="markup--strong markup--pre-strong">Residual sum of squares (MSE):</strong> %.2f&quot; % np.mean((test_y_hat - y_test) ** 2))</pre><pre name="711d" id="711d" class="graf graf--pre graf-after--pre">print(&quot;<strong class="markup--strong markup--pre-strong">Accuracy of train dataset is :</strong> &quot;,lm.score(x_train,y_train))</pre><pre name="1c3a" id="1c3a" class="graf graf--pre graf-after--pre">print(&quot;<strong class="markup--strong markup--pre-strong">Accuracy of test dataset is :</strong> &quot;,lm.score(x_test,y_test))</pre><h3 name="740f" id="740f" class="graf graf--h3 graf-after--pre">EVALUATION:</h3><figure name="b89a" id="b89a" class="graf graf--figure graf-after--h3"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 449px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 64.1%;"></div><img class="graf-image" data-image-id="0*rHyHeOQUiIiDNhq6.png" data-width="883" data-height="566" src="https://cdn-images-1.medium.com/max/800/0*rHyHeOQUiIiDNhq6.png"></div><figcaption class="imageCaption"><strong class="markup--strong markup--figure-strong">Here et = test_y_hat — y_test</strong></figcaption></figure><h3 name="8004" id="8004" class="graf graf--h3 graf-after--figure">Multivariable regression</h3><p name="a5a9" id="a5a9" class="graf graf--p graf-after--h3">Let’s say we are given data on TV, radio, and newspaper advertising spend for a list of companies, and our goal is to predict sales in terms of units sold.</p><figure name="3cc0" id="3cc0" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 648px; max-height: 257px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 39.7%;"></div><img class="graf-image" data-image-id="1*ipw2rzc7eP8umOkYDbystQ.png" data-width="648" data-height="257" src="https://cdn-images-1.medium.com/max/800/1*ipw2rzc7eP8umOkYDbystQ.png"></div><figcaption class="imageCaption">DataFrame</figcaption></figure><h3 name="3359" id="3359" class="graf graf--h3 graf-after--figure">Normalization</h3><p name="b19c" id="b19c" class="graf graf--p graf-after--h3">As the number of features grows, calculating gradient takes longer to compute. We can speed this up by “normalizing” our input data to ensure all values are within the same range. This is especially important for datasets with high standard deviations or differences in the ranges of the attributes. Our goal now will be to normalize our features so they are all in the range -1 to 1.</p><h3 name="601e" id="601e" class="graf graf--h3 graf-after--p">Making Prediction</h3><p name="4c85" id="4c85" class="graf graf--p graf-after--h3">Our predict function outputs an estimate of sales given our current weights (coefficients) and a company’s TV, radio, and newspaper spend. Our model will try to identify weight values that most reduce our cost function.</p><p name="1c28" id="1c28" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Sales=W1TV+W2Radio+W3Newspaper</strong></p><h3 name="8741" id="8741" class="graf graf--h3 graf-after--p">Cost Function</h3><p name="a484" id="a484" class="graf graf--p graf-after--h3">Now we need a cost function to audit how our model is performing. The math is the same, except we swap the mx+b expression for W1x1+W2x2+W3x3. We also divide the expression by 2 to make derivative calculations simpler.</p><pre name="9be8" id="9be8" class="graf graf--pre graf-after--p">MSE=(1/2N) (∑i=1n(yi−(W1x1+W2x2+W3x3))^2)</pre><h3 name="5595" id="5595" class="graf graf--h3 graf-after--pre">Gradient Descent</h3><p name="1a91" id="1a91" class="graf graf--p graf-after--h3">Again using the Chain Rule we can compute the gradient–a vector of partial derivatives describing the slope of the cost function for each weight.</p><pre name="f004" id="f004" class="graf graf--pre graf-after--p">f′(W1)=−x1(y−(W1x1+W2x2+W3x3))<br>f′(W2)=−x2(y−(W1x1+W2x2+W3x3))<br>f′(W3)=−x3(y−(W1x1+W2x2+W3x3))</pre><h3 name="ca32" id="ca32" class="graf graf--h3 graf-after--pre">Simplifying the Matrix</h3><p name="0131" id="0131" class="graf graf--p graf-after--h3">The gradient descent code above has a lot of duplication. Can we improve it somehow? One way to refactor would be to loop through our features and weights–allowing our function to handle any number of features. However there is another even better technique: <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">vectorized gradient descent</em></strong>.</p><p name="e54d" id="e54d" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Math</strong></p><p name="fc36" id="fc36" class="graf graf--p graf-after--p">We use the same formula as above, but instead of operating on a single feature at a time, we use <strong class="markup--strong markup--p-strong">matrix multiplication</strong> to operative on all features and weights simultaneously. We replace the xi terms with a single feature matrix X.</p><p name="1731" id="1731" class="graf graf--p graf-after--p">gradient=−X(targets−predictions)</p><p name="aaee" id="aaee" class="graf graf--p graf-after--p">These is how the math works in multivariable regression. Understanding this much is more than enough, because we never use it directly in our code. We use if directly using scikit-learn library. The code for these is completely similar to simple linear regression model discussed ahead.</p><p name="e7f5" id="e7f5" class="graf graf--p graf-after--p">I hope you liked this blog. Please make a feedback response.</p><p name="47d4" id="47d4" class="graf graf--p graf-after--p graf--trailing">For further blogs on this topic, make sure you follow me. Hope, you have a Great Day. BYE!!!</p></div></div></section>
</section><!--
<footer><p>By <a href="https://medium.com/@deeppatel23" class="p-author h-card">Deep Patel</a> on <a href="https://medium.com/p/e75dbae11937"><time class="dt-published" datetime="2020-06-26T09:55:52.888Z">June 26, 2020</time></a>.</p><p><a href="https://medium.com/@deeppatel23/regression-data-science-part-6-linear-regression-with-math-6-1-e75dbae11937" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on September 4, 2020.</p></footer>--></article></body></html>