<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Ridge &amp; Lasso Regression Technique(with Math)</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Ridge &amp; Lasso Regression Technique(with Math)</h1>
</header>
<section data-field="subtitle" class="p-summary">
Is Ridge &amp; Lasso same? Why we need this REGRESSION techniques?
</section>
<section data-field="body" class="e-content">
<section name="bd76" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="8d1d" id="8d1d" class="graf graf--h3 graf--leading graf--title">Ridge &amp; Lasso Regression Technique(with Math) for Data Science (Part 6.2)</h3><figure name="521a" id="521a" class="graf graf--figure graf--layoutOutsetLeft graf-after--h3"><div class="aspectRatioPlaceholder is-locked" style="max-width: 411px; max-height: 266px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 64.7%;"></div><img class="graf-image" data-image-id="0*FYBmKsm6wuTICSmL.png" data-width="411" data-height="266" data-is-featured="true" src="https://cdn-images-1.medium.com/max/600/0*FYBmKsm6wuTICSmL.png"></div><figcaption class="imageCaption"><strong class="markup--strong markup--figure-strong">Linear</strong> VS <strong class="markup--strong markup--figure-strong">Ridge</strong> VS <strong class="markup--strong markup--figure-strong">Lasso</strong> REGRESSION</figcaption></figure><p name="c720" id="c720" class="graf graf--p graf-after--figure">We already learned <a href="https://medium.com/@deeppatel23/regression-data-science-part-6-linear-regression-with-math-6-1-e75dbae11937" data-href="https://medium.com/@deeppatel23/regression-data-science-part-6-linear-regression-with-math-6-1-e75dbae11937" class="markup--anchor markup--p-anchor" target="_blank">Linear Regression</a>, so what’s new here? What was the need for Ridge &amp; Lasso Regression technique?</p><p name="2feb" id="2feb" class="graf graf--p graf-after--p">We will be answering all these questions right here with detailed explanation.</p><h3 name="c39c" id="c39c" class="graf graf--h3 graf-after--p">First, Let&#39;s look at possible results that can be obtained.</h3><figure name="ee48" id="ee48" class="graf graf--figure graf-after--h3"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 243px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 34.8%;"></div><img class="graf-image" data-image-id="0*fWnxPH8-HEUCetPz.png" data-width="1125" data-height="391" src="https://cdn-images-1.medium.com/max/800/0*fWnxPH8-HEUCetPz.png"></div><figcaption class="imageCaption"><strong class="markup--strong markup--figure-strong">Under</strong>fitting VS <strong class="markup--strong markup--figure-strong">Good</strong> Fit(Generalized) VS <strong class="markup--strong markup--figure-strong">Over</strong>fitting</figcaption></figure><p name="3699" id="3699" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">Underfitting</strong> occurs when the model doesn&#39;t work well with both training data and testing data (meaning, accuracy of both training &amp; testing dataset is below 50%). <strong class="markup--strong markup--p-strong">Possible solution</strong> is applying Data Wrangling (data preprocessing or feature engineering).</p><p name="89ab" id="89ab" class="graf graf--p graf-after--p">A model is a <strong class="markup--strong markup--p-strong">Good Fit</strong> when it works well with both training and testing dataset (meaning, accuracy of both dataset is around 70%–85%). It mean that we almost achieved our goal.</p><p name="c5fa" id="c5fa" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Overfitting</strong> occurs when the model works very well with training data, but when it comes to testing data it fails (meaning, accuracy of training data is &gt;90% and testing data is &lt; 65%). Since model works best at training data, so whenever it faces a new situation during testing it gives wrong results. It is also called a model with <strong class="markup--strong markup--p-strong">high variance</strong>. <strong class="markup--strong markup--p-strong">Possible solution</strong> is to use correct regression technique, that is <strong class="markup--strong markup--p-strong">Ridge </strong>or<strong class="markup--strong markup--p-strong"> Lasso</strong>.</p><h3 name="5341" id="5341" class="graf graf--h3 graf-after--p">How Ridge Regression Works? How it solves Overfitting?</h3><p name="2f86" id="2f86" class="graf graf--p graf-after--h3">Ridge regression add one more term to Linear regression’s cost function. The primary reason why these penalty terms are added is two ensure there is <strong class="markup--strong markup--p-strong">regularization</strong>, shrinking the weights of the model to zero or close to zero to ensure that the model does not overfit the data.</p><p name="5d14" id="5d14" class="graf graf--p graf-after--p">In the context of <strong class="markup--strong markup--p-strong">machine learning</strong>, <strong class="markup--strong markup--p-strong">regularization</strong> is the process which regularizes or shrinks the coefficients towards zero. In simple words, <strong class="markup--strong markup--p-strong">regularization</strong> discourages <strong class="markup--strong markup--p-strong">learning</strong> a more complex or flexible model, to prevent overfitting. Let&#39;s look at a cost function for better understanding.</p><figure name="76d6" id="76d6" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 141px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 20.1%;"></div><img class="graf-image" data-image-id="0*WgFQ2plcWQuveSGo.png" data-width="1337" data-height="269" src="https://cdn-images-1.medium.com/max/800/0*WgFQ2plcWQuveSGo.png"></div><figcaption class="imageCaption"><strong class="markup--strong markup--figure-strong">RSS</strong> stands for Residual Sum of Squares.</figcaption></figure><p name="1c33" id="1c33" class="graf graf--p graf-after--figure">The <strong class="markup--strong markup--p-strong">R</strong>esidual <strong class="markup--strong markup--p-strong">S</strong>um of <strong class="markup--strong markup--p-strong">S</strong>quares measures the amount of error remaining between the <strong class="markup--strong markup--p-strong">regression</strong> function and the data set. It is a measure of amount of statistical variance present in dataset.</p><figure name="5571" id="5571" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 390px; max-height: 258px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 66.2%;"></div><img class="graf-image" data-image-id="0*wve3v-_HMf8c3hiB.png" data-width="390" data-height="258" src="https://cdn-images-1.medium.com/max/800/0*wve3v-_HMf8c3hiB.png"></div><figcaption class="imageCaption">Computing these distance, we get <strong class="markup--strong markup--figure-strong">Variance</strong>.</figcaption></figure><p name="b9d3" id="b9d3" class="graf graf--p graf-after--figure">For fixed values of <strong class="markup--strong markup--p-strong">lambda</strong> in the second term, the multiplication of lambda along with <em class="markup--em markup--p-em">c </em>yields a constant term. So essentially we will be minimizing the equation we have for ridge above. Lambda(or <strong class="markup--strong markup--p-strong">alpha</strong>) is a <strong class="markup--strong markup--p-strong">hyper-parameter</strong> that we tune and we set it to a particular value based on our choice. If it is set to zero then the equation of ridge gets converted to that of normal linear regression. The value of lambda will be chosen by <strong class="markup--strong markup--p-strong">cross-validation</strong>.</p><p name="fa5b" id="fa5b" class="graf graf--p graf-after--p">α can take various values:</p><p name="1d04" id="1d04" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">α = 0:</strong></p><ul class="postList"><li name="b18e" id="b18e" class="graf graf--li graf-after--p">The objective becomes same as simple linear regression.</li><li name="54e0" id="54e0" class="graf graf--li graf-after--li">We’ll get the same coefficients as simple linear regression.</li></ul><p name="d10c" id="d10c" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">α = ∞:</strong></p><ul class="postList"><li name="4bfa" id="4bfa" class="graf graf--li graf-after--p">The coefficients will be zero. Why? Because of infinite weightage on square of coefficients, anything less than zero will make the objective infinite.</li></ul><p name="34be" id="34be" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">0 &lt; α &lt; ∞:</strong></p><ul class="postList"><li name="043c" id="043c" class="graf graf--li graf-after--p">The magnitude of α will decide the weightage given to different parts of objective.</li><li name="a222" id="a222" class="graf graf--li graf-after--li">The coefficients will be somewhere between 0 and ones for simple linear regression.</li></ul><p name="78fc" id="78fc" class="graf graf--p graf-after--li">I hope this gives some sense on how α would impact the magnitude of coefficients. The plot shows cross-validated mean squared error. As lambda decreases, the mean squared error decreases.</p><figure name="bd0c" id="bd0c" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 452px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 64.60000000000001%;"></div><img class="graf-image" data-image-id="1*tpxk_HFQrP1NlzcPdl7PJA.png" data-width="969" data-height="626" src="https://cdn-images-1.medium.com/max/800/1*tpxk_HFQrP1NlzcPdl7PJA.png"></div><figcaption class="imageCaption"><strong class="markup--strong markup--figure-strong">Ridge includes all the variables in the model and the value of lambda selected is indicated by the vertical lines.</strong></figcaption></figure><p name="304d" id="304d" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">Cross validation</strong> train the algorithm on a training dataset, and then runs the trained algorithm on a validation set. Lambda values are determined by reducing the percentage of errors of the trained algorithm on the validation set. Overall, choosing a proper value of Lambda for ridge regression allows it to properly fit data in machine learning tasks that use ill-posed problems.</p><p name="1390" id="1390" class="graf graf--p graf-after--p">This will eventually reduce overfitting, though our model will won’t perform well with training data, but if will be more generalized, and also it will gives better results on test dataset.</p><pre name="d556" id="d556" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">from</strong> <strong class="markup--strong markup--pre-strong">sklearn.linear_model</strong> <strong class="markup--strong markup--pre-strong">import</strong> Ridge </pre><pre name="8bb5" id="8bb5" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">#Grid search is an approach to parameter tuning that will methodically build and evaluate a model for each combination of algorithm parameters specified in a grid.</strong><br><strong class="markup--strong markup--pre-strong">from</strong> <strong class="markup--strong markup--pre-strong">sklearn.model_selection</strong> <strong class="markup--strong markup--pre-strong">import</strong> GridSearchCV  </pre><pre name="ccff" id="ccff" class="graf graf--pre graf-after--pre">ridge=Ridge()</pre><pre name="c06c" id="c06c" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">#Here alpha is lambda: is the parameter which balances the amount of emphasis given to minimizing RSS vs minimizing sum of square of coefficients</strong></pre><pre name="3d49" id="3d49" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong"><em class="markup--em markup--pre-em">#These values of alpha have been chosen so that we can easily analyze the trend with change. These would however differ from case to case.</em></strong><br>parameters={&#39;alpha&#39;:[1e-15, 1e-10, 1e-8, 1e-3, 1e-2, 1, 5, 10, 20, 30, 35, 40, 45, 50, 55, 100]}</pre><pre name="9133" id="9133" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">ridge_regressor</strong>=GridSearchCV(ridge,parameters,scoring=&#39;neg_mean_squared_error&#39;,cv=5) ridge_regressor.fit(X,y)</pre><pre name="e7e3" id="e7e3" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong"><em class="markup--em markup--pre-em">#Shows the best value of alpha that fits model</em></strong><br>print(ridge_regressor.best_params_)</pre><pre name="282a" id="282a" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong"><em class="markup--em markup--pre-em">#Greater the value better the results</em></strong><br>print(ridge_regressor.best_score_)</pre><h3 name="4624" id="4624" class="graf graf--h3 graf-after--pre">Now, Overfitting is also solved, so what&#39;s new Lasso does?</h3><p name="5adf" id="5adf" class="graf graf--p graf-after--h3"><strong class="markup--strong markup--p-strong">Lasso Regression :</strong> LASSO stands for <strong class="markup--strong markup--p-strong">L</strong>east <strong class="markup--strong markup--p-strong">A</strong>bsolute <strong class="markup--strong markup--p-strong">S</strong>hrinkage and <strong class="markup--strong markup--p-strong">S</strong>election <strong class="markup--strong markup--p-strong">O</strong>perator. I know it doesn’t give much of an idea but there are 2 key words here — “<strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">absolute</em></strong><em class="markup--em markup--p-em">”</em> and “<strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">selection</em></strong><em class="markup--em markup--p-em">”</em>.</p><p name="8dfe" id="8dfe" class="graf graf--p graf-after--p">The cost function for Lasso (<strong class="markup--strong markup--p-strong">L</strong>east <strong class="markup--strong markup--p-strong">A</strong>bsolute <strong class="markup--strong markup--p-strong">S</strong>hrinkage and <strong class="markup--strong markup--p-strong">S</strong>election <strong class="markup--strong markup--p-strong">O</strong>perator) regression can be written as:</p><figure name="8067" id="8067" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 128px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 18.3%;"></div><img class="graf-image" data-image-id="1*Zo5zM5PJyjDY6m_fqD8U7g.png" data-width="764" data-height="140" src="https://cdn-images-1.medium.com/max/800/1*Zo5zM5PJyjDY6m_fqD8U7g.png"></div><figcaption class="imageCaption"><strong class="markup--strong markup--figure-strong">COST</strong> Function for <strong class="markup--strong markup--figure-strong">L</strong>asso Regression</figcaption></figure><p name="cfb4" id="cfb4" class="graf graf--p graf-after--figure">Thus, from observing the cost function, the only <strong class="markup--strong markup--p-strong">difference</strong> found between <strong class="markup--strong markup--p-strong">R</strong>idge &amp; <strong class="markup--strong markup--p-strong">L</strong>asso is that: <em class="markup--em markup--p-em">instead of taking the square of the coefficients, magnitudes are taken into account</em>. This type of regularization can lead to zero coefficients i.e. some of the features are completely neglected for the evaluation of output. <strong class="markup--strong markup--p-strong">So Lasso regression not only helps in reducing over-fitting but it can help us in feature selection. </strong>Ridge regression only reduces the coefficients close to zero, but not zero; whereas Lasso regression can reduce coefficient to zero, thus result in feature selection.<strong class="markup--strong markup--p-strong"> </strong>Same as in ridge regression, here also the hyperparameter Lambda can be controlled and all other functioning works the same.</p><pre name="8ad0" id="8ad0" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">from</strong> <strong class="markup--strong markup--pre-strong">sklearn.linear_model</strong> <strong class="markup--strong markup--pre-strong">import</strong> Lasso <br><strong class="markup--strong markup--pre-strong">from</strong> <strong class="markup--strong markup--pre-strong">sklearn.model_selection</strong> <strong class="markup--strong markup--pre-strong">import</strong> GridSearchCV lasso=Lasso() parameters={&#39;alpha&#39;:[1e-15, 1e-10, 1e-8, 1e-3, 1e-2, 1, 5, 10, 20, 30, 35, 40, 45, 50, 55, 100]} </pre><pre name="90cc" id="90cc" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">lasso_regressor</strong>=GridSearchCV(lasso,parameters,scoring=&#39;neg_mean_squared_error&#39;,cv=5)  lasso_regressor.fit(X,y) </pre><pre name="0c05" id="0c05" class="graf graf--pre graf-after--pre">print(lasso_regressor.best_params_) print(lasso_regressor.best_score_)</pre><p name="087d" id="087d" class="graf graf--p graf-after--pre">Finally to end this meditation, let’s summarize what we have learnt so far</p><ol class="postList"><li name="6f7f" id="6f7f" class="graf graf--li graf-after--p">Cost function of Ridge and Lasso regression and importance of regularization term.</li><li name="33b8" id="33b8" class="graf graf--li graf-after--li">They both try to reduce the coefficient to zero to generalize the model.</li><li name="7e2b" id="7e2b" class="graf graf--li graf-after--li">Lasso regression can lead to feature selection whereas Ridge can only shrink coefficients close to zero.</li></ol><p name="7005" id="7005" class="graf graf--p graf-after--li">NOTE: Based on my experience, Ridge regression performs better than Lasso regression in most of cases. Try to use Lasso regression only when there are too many features.</p><p name="d2e0" id="d2e0" class="graf graf--p graf-after--p graf--trailing">In the next blog I will be uploading on polynomial and some other regression technique to complete this topic to it fullest. Make sure you follow me to read each and every blog. Hope you have enjoyed the post and stay happy ! <strong class="markup--strong markup--p-strong">Cheers !</strong></p></div></div></section>
</section><!--
<footer><p>By <a href="https://medium.com/@deeppatel23" class="p-author h-card">Deep Patel</a> on <a href="https://medium.com/p/ead6246abd8b"><time class="dt-published" datetime="2020-06-27T05:05:46.395Z">June 27, 2020</time></a>.</p><p><a href="https://medium.com/@deeppatel23/ridge-lasso-regression-technique-with-math-for-data-science-part-6-2-ead6246abd8b" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on September 4, 2020.</p></footer>--></article></body></html>