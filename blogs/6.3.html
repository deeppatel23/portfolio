<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Polynomial | Stepwise | ElasticNet REGRESSIONS</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Polynomial | Stepwise | ElasticNet REGRESSIONS</h1>
</header>
<section data-field="subtitle" class="p-summary">
Different regressions for different purposes.
</section>
<section data-field="body" class="e-content">
<section name="2682" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="e373" id="e373" class="graf graf--h3 graf--leading graf--title">Polynomial, Stepwise &amp; ElasticNet Regression for Data Science(Part 6.3)</h3><p name="5086" id="5086" class="graf graf--p graf-after--h3">Starting with the <strong class="markup--strong markup--p-strong">Polynomial Regression</strong>, this graph says much of it.</p><figure name="10fd" id="10fd" class="graf graf--figure graf--layoutOutsetLeft graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 525px; max-height: 337px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 64.2%;"></div><img class="graf-image" data-image-id="0*V-dgpN75uIUh8Iq8" data-width="909" data-height="584" data-is-featured="true" src="https://cdn-images-1.medium.com/max/600/0*V-dgpN75uIUh8Iq8"></div><figcaption class="imageCaption"><strong class="markup--strong markup--figure-strong">Green</strong>-Linear, <strong class="markup--strong markup--figure-strong">Red</strong>-Polynomial; Image credit <a href="https://medium.com/u/8214a51445b5" data-href="https://medium.com/u/8214a51445b5" data-anchor-type="2" data-user-id="8214a51445b5" data-action-value="8214a51445b5" data-action="show-user-card" data-action-type="hover" class="markup--user markup--figure-user" target="_blank">Deep¬†Patel</a></figcaption></figure><p name="d117" id="d117" class="graf graf--p graf-after--figure">What if your linear regression model cannot model the relationship between the target variable and the predictor variable<strong class="markup--strong markup--p-strong">?</strong> In other words, what if they don‚Äôt have a linear relationship<strong class="markup--strong markup--p-strong">?</strong></p><p name="6909" id="6909" class="graf graf--p graf-after--p">After this blog, you will definitely get all your answers.</p><p name="eb80" id="eb80" class="graf graf--p graf-after--p">This is my third blog on Regression series. This blog requires prior knowledge of <a href="https://medium.com/@deeppatel23/regression-data-science-part-6-linear-regression-with-math-6-1-e75dbae11937" data-href="https://medium.com/@deeppatel23/regression-data-science-part-6-linear-regression-with-math-6-1-e75dbae11937" class="markup--anchor markup--p-anchor" target="_blank">Linear Regression</a>.</p><h3 name="c88e" id="c88e" class="graf graf--h3 graf-after--p">What is Polynomial Regression?</h3><p name="3422" id="3422" class="graf graf--p graf-after--h3">Polynomial regression is a special case of linear regression where we fit a polynomial equation on the data with a <strong class="markup--strong markup--p-strong">curvilinear relationship</strong> between the target variable and the independent variables.</p><p name="8aca" id="8aca" class="graf graf--p graf-after--p">As we can see in the above image that the straight line doesn‚Äôt fit the data at all. This arises the case of underfitting. To overcome this underfitting, we need to increase the complexity. Thus, the need of these regression came into existence when we found a type of completely <strong class="markup--strong markup--p-strong">non-linear relation</strong>.</p><p name="3599" id="3599" class="graf graf--p graf-after--p">The implementation of polynomial regression is a <strong class="markup--strong markup--p-strong">two-step</strong> process. First, we transform our data into a polynomial using the <strong class="markup--strong markup--p-strong">Polynomial Features</strong> function from sklearn and then use linear regression to fit the parameters:</p><figure name="47fe" id="47fe" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 138px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 19.7%;"></div><img class="graf-image" data-image-id="0*aSoVtI3lm6sGh4Sy.png" data-width="771" data-height="152" src="https://cdn-images-1.medium.com/max/800/0*aSoVtI3lm6sGh4Sy.png"></div><figcaption class="imageCaption">Complete Pipeline</figcaption></figure><p name="ab66" id="ab66" class="graf graf--p graf-after--figure">In a curvilinear relationship, the value of the target variable changes in a non-uniform manner with respect to the predictor (s). In polynomial regression, we have a polynomial equation of degree <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">n </em></strong>represented as:</p><figure name="15ab" id="15ab" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 580px; max-height: 68px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 11.700000000000001%;"></div><img class="graf-image" data-image-id="1*a1hfS1BbtHoGJi_1qkCNUQ.png" data-width="580" data-height="68" src="https://cdn-images-1.medium.com/max/800/1*a1hfS1BbtHoGJi_1qkCNUQ.png"></div><figcaption class="imageCaption">A Polynomial Equation</figcaption></figure><p name="3a3c" id="3a3c" class="graf graf--p graf-after--figure">Here: <strong class="markup--strong markup--p-strong">ùúÉ0</strong> is the bias, <strong class="markup--strong markup--p-strong">ùúÉ1, ùúÉ2,¬†‚Ä¶, ùúÉn</strong> are the weights in the equation of the polynomial regression, and <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">n</em></strong> is the degree of the polynomial. The number of higher-order terms increases with the increasing value of <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">n</em></strong><em class="markup--em markup--p-em">,</em> and hence the equation becomes more complicated. Note that n must be greater than 1, because n=1 is a simple linear regression.</p><p name="fbdb" id="fbdb" class="graf graf--p graf-after--p">For <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">n</em></strong> predictors, the equation includes all the possible combinations of different order polynomials. This is known as <strong class="markup--strong markup--p-strong">Multi-dimensional Polynomial Regression</strong>.</p><p name="9fc4" id="9fc4" class="graf graf--p graf-after--p">But, there is a major <strong class="markup--strong markup--p-strong">issue</strong> with multi-dimensional Polynomial Regression‚Ää‚Äî‚Äämulticollinearity. <strong class="markup--strong markup--p-strong">Multicollinearity</strong> occurs when independent variables in a regression model are <strong class="markup--strong markup--p-strong">correlated.</strong> This correlation is a problem because independent variables should be independent. If the degree of correlation between variables is high enough, it can cause problems when you fit the model and interpret the results. This restricts the model from fitting properly on the dataset.</p><p name="730c" id="730c" class="graf graf--p graf-after--p">With the increasing degree of the polynomial, the complexity of the model also increases. Therefore, the value of <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">n</em></strong> must be chosen precisely. If this value is low, then the model won‚Äôt be able to fit the data properly and if high, the model will overfit the data easily.</p><pre name="7977" id="7977" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">#Import necessary libraties</strong><br>from sklearn.linear_model import LinearRegressionfrom sklearn.metrics import mean_squared_error, r2_scorefrom sklearn.preprocessing import PolynomialFeatures</pre><pre name="6af1" id="6af1" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">#Change degree to reduce the error</strong><br>polynomial_features= PolynomialFeatures(degree=2)<br>x_poly = polynomial_features.fit_transform(x_train)</pre><pre name="6b32" id="6b32" class="graf graf--pre graf-after--pre">model = LinearRegression()<br>model.fit(x_poly, y)y_poly_pred = model.predict(x_poly)</pre><pre name="df95" id="df95" class="graf graf--pre graf-after--pre">rmse = np.sqrt(mean_squared_error(y,y_poly_pred))<br>r2 = r2_score(y,y_poly_pred)<br>print(rmse)<br>print(r2)</pre><pre name="c307" id="c307" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">#Plotting the polynomial line</strong><br>plt.scatter(x, y, s=10)<br># sort the values of x before line plot<br>sort_axis = operator.itemgetter(0)<br>sorted_zip = sorted(zip(x,y_poly_pred), key=sort_axis)<br>x, y_poly_pred = zip(*sorted_zip)<br>plt.plot(x, y_poly_pred, color=&#39;m&#39;)<br>plt.show()</pre><p name="4616" id="4616" class="graf graf--p graf-after--pre">Finally, trying multiple times, you will get something like this:</p><figure name="750d" id="750d" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 557px; max-height: 409px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 73.4%;"></div><img class="graf-image" data-image-id="1*Onu53K9W7e1qB5MBUCqTqA.png" data-width="557" data-height="409" src="https://cdn-images-1.medium.com/max/800/1*Onu53K9W7e1qB5MBUCqTqA.png"></div><figcaption class="imageCaption">Visualizing the fit with different <strong class="markup--strong markup--figure-strong">degree</strong></figcaption></figure><p name="f15b" id="f15b" class="graf graf--p graf-after--figure">Things to be kept in mind for better results.</p><h3 name="beb2" id="beb2" class="graf graf--h3 graf-after--p">The Bias vs Variance trade-off</h3><p name="9d59" id="9d59" class="graf graf--p graf-after--h3"><strong class="markup--strong markup--p-strong">Bias </strong>refers to the error due to the model‚Äôs simplistic assumptions in fitting the data. A high bias means that the model is unable to capture the patterns in the data and this results in <strong class="markup--strong markup--p-strong">under-fitting</strong>.</p><p name="1307" id="1307" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Variance </strong>refers to the error due to the complex model trying to fit the data. High variance means the model passes through most of the data points and it results in <strong class="markup--strong markup--p-strong">over-fitting</strong> the data.</p><p name="6322" id="6322" class="graf graf--p graf-after--p">The below picture summarizes our learning.</p><figure name="8290" id="8290" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 299px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 42.699999999999996%;"></div><img class="graf-image" data-image-id="1*tjkallC7LhhrmO05NUv5RA.png" data-width="979" data-height="418" src="https://cdn-images-1.medium.com/max/800/1*tjkallC7LhhrmO05NUv5RA.png"></div><figcaption class="imageCaption">Illness of¬†model</figcaption></figure><p name="2f74" id="2f74" class="graf graf--p graf-after--figure">From the below picture we can observe that as the model complexity increases, the bias decreases and the variance increases and vice-versa. Ideally, a machine learning model should have <strong class="markup--strong markup--p-strong">low variance and low bias</strong>. But practically it‚Äôs impossible to have both. Therefore to achieve a good model that performs well both on the train and unseen data, a <strong class="markup--strong markup--p-strong">trade-off</strong> is made. Trade-off is tension between the error introduced by the bias and the variance.</p><figure name="9c88" id="9c88" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 492px; max-height: 309px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 62.8%;"></div><img class="graf-image" data-image-id="0*FN3Bsdjre_OWSs_K.png" data-width="492" data-height="309" src="https://cdn-images-1.medium.com/max/800/0*FN3Bsdjre_OWSs_K.png"></div><figcaption class="imageCaption">Source: <a href="http://scott.fortmann-roe.com/docs/BiasVariance.html" data-href="http://scott.fortmann-roe.com/docs/BiasVariance.html" class="markup--anchor markup--figure-anchor" rel="noopener nofollow noopener" target="_blank">http://scott.fortmann-roe.com/docs/BiasVariance.html</a></figcaption></figure><p name="156e" id="156e" class="graf graf--p graf-after--figure">This was all about Polynomial Regression. GO ahead in this blog for other regression model.</p><h3 name="96d3" id="96d3" class="graf graf--h3 graf-after--p">Stepwise regression</h3><figure name="9c54" id="9c54" class="graf graf--figure graf-after--h3"><div class="aspectRatioPlaceholder is-locked" style="max-width: 500px; max-height: 307px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 61.4%;"></div><img class="graf-image" data-image-id="0*OiT-blJYdH0RVAVz.jpg" data-width="500" data-height="307" src="https://cdn-images-1.medium.com/max/800/0*OiT-blJYdH0RVAVz.jpg"></div><figcaption class="imageCaption">Image credit: <a href="https://en.wikipedia.org/wiki/Stepwise_regression" data-href="https://en.wikipedia.org/wiki/Stepwise_regression" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">https://en.wikipedia.org/wiki/Stepwise_regression</a></figcaption></figure><p name="1ca8" id="1ca8" class="graf graf--p graf-after--figure">It is highly used to meet regression models with predictive models that are carried out naturally. With every forward step, the variable gets added or subtracted from a group of descriptive variables.</p><p name="0eb4" id="0eb4" class="graf graf--p graf-after--p">The formula for stepwise regression is:</p><figure name="fe08" id="fe08" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 327px; max-height: 55px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 16.8%;"></div><img class="graf-image" data-image-id="1*Kk9WHBB2VD5pjqBFQXt4Ww.png" data-width="327" data-height="55" src="https://cdn-images-1.medium.com/max/800/1*Kk9WHBB2VD5pjqBFQXt4Ww.png"></div><figcaption class="imageCaption">Formula</figcaption></figure><p name="900b" id="900b" class="graf graf--p graf-after--figure">The criteria followed in the stepwise regression technique are forward determination (<strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">forward selection</em></strong>), backward exclusion (<strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">backward elimination</em></strong>), and bidirectional removal (<strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">bidirectional elimination</em></strong>).</p><p name="bba5" id="bba5" class="graf graf--p graf-after--p">Forward selection performs the continuously adding variables in order to review the performance and stopped when no improvement is needed up to an extent. Backward elimination includes the removal of variables at a time until no extra variables would be deleted without considerable loss. And bidirectional elimination is the blend of the above two approaches.</p><h3 name="27db" id="27db" class="graf graf--h3 graf-after--p">ElasticNet Regression</h3><p name="bb2b" id="bb2b" class="graf graf--p graf-after--h3">It is the mixture of ridge and lasso regression that brings out a grouping effect when highly correlated predictors approach to be in or out in the model combinedly. It is recommended to use when the number of predictors is very greater than the number of observations. It is uses both the L1 and L2 regularization taking on the effects of both techniques:</p><figure name="64fa" id="64fa" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 143px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 20.4%;"></div><img class="graf-image" data-image-id="0*m18a9ehC_oquT6tq.png" data-width="2100" data-height="428" src="https://cdn-images-1.medium.com/max/800/0*m18a9ehC_oquT6tq.png"></div><figcaption class="imageCaption"><strong class="markup--strong markup--figure-strong">Elastic Net = RSS + L1 +¬†L2</strong></figcaption></figure><p name="9968" id="9968" class="graf graf--p graf-after--figure">A practical advantage of trading-off between Lasso and Ridge is that, it allows Elastic-Net to inherit some of Ridge‚Äôs stability under rotation.</p><p name="550b" id="550b" class="graf graf--p graf-after--p">A few key points about ElasticNet Regression:</p><ul class="postList"><li name="3c4f" id="3c4f" class="graf graf--li graf-after--p">It encourages group effect in the case of highly correlated variables, rather than zeroing some of them out like Lasso.</li><li name="bbf0" id="bbf0" class="graf graf--li graf-after--li">There are no limitations on the number of selected variables.</li></ul><h3 name="0722" id="0722" class="graf graf--h3 graf-after--li">CONCLUSION:</h3><p name="5e27" id="5e27" class="graf graf--p graf-after--h3">There you have it, we have discussed the 6 most common types of regression analysis that are important in Data Science and Machine Learning (ML). In the nutshell, regression analysis is a set of statistical tools and methods that enables one to formulate a predicted mathematical equation between the creative effects and performance outcomes and shows the casual-effect connection.</p><p name="0d16" id="0d16" class="graf graf--p graf-after--p">Moreover, the selection of picking the right regression technique entirely depends on the data and requirements needed to apply. I expect you have savored studying these three blogs(<a href="https://medium.com/@deeppatel23/regression-data-science-part-6-linear-regression-with-math-6-1-e75dbae11937" data-href="https://medium.com/@deeppatel23/regression-data-science-part-6-linear-regression-with-math-6-1-e75dbae11937" class="markup--anchor markup--p-anchor" target="_blank">6.1</a>, <a href="https://medium.com/@deeppatel23/ridge-lasso-regression-technique-with-math-for-data-science-part-6-2-ead6246abd8b" data-href="https://medium.com/@deeppatel23/ridge-lasso-regression-technique-with-math-for-data-science-part-6-2-ead6246abd8b" class="markup--anchor markup--p-anchor" target="_blank">6.2</a>, 6.3) and surely receive something new. For more blogs follow me‚Ä¶</p><p name="bfe9" id="bfe9" class="graf graf--p graf-after--p graf--trailing">I hope that you like this blog. Make sure you follow me to read each and every blog. Hope you have enjoyed the post and stay happy¬†! Cheers¬†!Thank You!!!</p></div></div></section>
</section><!--
<footer><p>By <a href="https://medium.com/@deeppatel23" class="p-author h-card">Deep Patel</a> on <a href="https://medium.com/p/6bed468ca5c4"><time class="dt-published" datetime="2020-06-27T08:21:13.734Z">June 27, 2020</time></a>.</p><p><a href="https://medium.com/@deeppatel23/polynomial-stepwise-elasticnet-regression-for-data-science-part-6-3-6bed468ca5c4" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on September 4, 2020.</p></footer>--></article></body></html>