<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Random Forest Classifier | Machine Learning</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Random Forest Classifier | Machine Learning</h1>
</header>
<section data-field="subtitle" class="p-summary">
Here, we discussed the random forest classifier technique with deep intuition and python implementation of various hyperparameters.
</section>
<section data-field="body" class="e-content">
<section name="9edc" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="7f3b" id="7f3b" class="graf graf--h3 graf--leading graf--title">Random Forest Classifier | Data Science | ML(Part 7.3)</h3><h4 name="3d5e" id="3d5e" class="graf graf--h4 graf-after--h3 graf--subtitle">Here, we discussed the random forest classifier technique with deep intuition and python implementation of various hyperparameters.</h4><p name="b9eb" id="b9eb" class="graf graf--p graf-after--h4">Hello &amp; Welcome!!</p><figure name="47fa" id="47fa" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 300px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 42.9%;"></div><img class="graf-image" data-image-id="0*eKoSgZc3WEkJQifg.jpg" data-width="980" data-height="420" src="https://cdn-images-1.medium.com/max/800/0*eKoSgZc3WEkJQifg.jpg"></div><figcaption class="imageCaption">Any Random Forest</figcaption></figure><p name="db62" id="db62" class="graf graf--p graf-after--figure">Similar to the forest of real life made of thousands of trees, Random Forest classifier is also made by combining multiple decision tree(not thousands) and taking mode as output.</p><p name="4ba2" id="4ba2" class="graf graf--p graf-after--p">Random forest is a supervised machine learning algorithm. The “forest” it builds, is an <strong class="markup--strong markup--p-strong">ensemble</strong> of decision trees, usually trained with the “<strong class="markup--strong markup--p-strong">bagging</strong>” method.</p><p name="a61c" id="a61c" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">Wait!!!</em></strong></p><p name="ab5f" id="ab5f" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">First we need to know what is ensemble &amp; wha</strong>t is <strong class="markup--strong markup--p-strong">bagging</strong>?</p><p name="2c84" id="2c84" class="graf graf--p graf-after--p">Just like you come to a decision to buy a car by reading multiple reviews and opinions, in machine learning also, you can combine the decisions from multiple models to improve the overall performance. This technique of combining multiple machine learning models is called <strong class="markup--strong markup--p-strong">ensemble learning</strong>. Ensemble learning is one of the most effective ways to build an efficient machine learning model. You can build an ensemble machine learning model using simple models and yet get great scores which are at par with the resource-hungry models like neural networks.</p><figure name="dc6d" id="dc6d" class="graf graf--figure graf--layoutOutsetLeft graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 525px; max-height: 492px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 93.7%;"></div><img class="graf-image" data-image-id="0*d3UBw4LhHGLOsMXY.png" data-width="756" data-height="708" src="https://cdn-images-1.medium.com/max/600/0*d3UBw4LhHGLOsMXY.png"></div><figcaption class="imageCaption">BAGGING</figcaption></figure><p name="9cfb" id="9cfb" class="graf graf--p graf-after--figure">The idea behind <strong class="markup--strong markup--p-strong">bagging</strong> (or <strong class="markup--strong markup--p-strong">Bootstrap Aggregating)</strong> is combining the results of multiple models (for instance, all decision trees) to get a generalized result. Bootstrapping is a sampling technique in which we create subsets of observations from the original dataset, <strong class="markup--strong markup--p-strong">with replacement</strong>. The size of subsets created for bagging may be less than the original set. Bagging technique uses subsets (bags) to get a fair idea of the distribution (complete set).</p><p name="4cce" id="4cce" class="graf graf--p graf-after--p">Bagging=Decision Tree + Row sampling</p><p name="940d" id="940d" class="graf graf--p graf-after--p">Bagging is the application of the Bootstrap procedure to a high-variance machine learning algorithm, typically decision trees.</p><blockquote name="a870" id="a870" class="graf graf--pullquote graf-after--p">In simple words <strong class="markup--strong markup--pullquote-strong">random forest builds multiple decision trees and merges them together to get a more accurate and stable prediction.</strong></blockquote><h3 name="6ae1" id="6ae1" class="graf graf--h3 graf-after--pullquote">FEATURE IMPORTANCE</h3><p name="fd39" id="fd39" class="graf graf--p graf-after--h3">Another great quality of the random forest algorithm is that it is very easy to measure the relative importance of each feature on the prediction. Sklearn provides a great tool for this that measures a feature’s importance by looking at how much the tree nodes that use that feature reduce impurity across all trees in the forest. It computes this score automatically for each feature after training and scales the results so the sum of all importance is equal to one.</p><p name="3571" id="3571" class="graf graf--p graf-after--p">If you don’t know how a decision tree works or what a leaf or node is, then read my <a href="https://medium.com/@deeppatel23/decision-tree-classifier-data-science-ml-part-7-2-4062186b8a9b" data-href="https://medium.com/@deeppatel23/decision-tree-classifier-data-science-ml-part-7-2-4062186b8a9b" class="markup--anchor markup--p-anchor" target="_blank">previous blog</a>. I have already explained complete working of decision tree over there.</p><p name="9693" id="9693" class="graf graf--p graf-after--p">By looking at the feature importance you can decide which features to possibly drop because they don’t contribute enough (or sometimes nothing at all) to the prediction process. This is important because a general rule in machine learning is that the more features you have the more likely your model will suffer from overfitting and vice versa.</p><p name="f1c6" id="f1c6" class="graf graf--p graf-after--p">Looking at it <strong class="markup--strong markup--p-strong">step-by-step</strong>, this is what a random forest model does:</p><ol class="postList"><li name="1287" id="1287" class="graf graf--li graf-after--p">Random subsets are created from the original dataset (bootstrapping).</li><li name="552c" id="552c" class="graf graf--li graf-after--li">At each node in the decision tree, only a random set of features are considered to decide the best split.</li><li name="ee3f" id="ee3f" class="graf graf--li graf-after--li">A decision tree model is fitted on each of the subsets.</li><li name="faeb" id="faeb" class="graf graf--li graf-after--li">The final prediction is calculated by averaging(or mode) the predictions from all decision trees.</li></ol><p name="8d16" id="8d16" class="graf graf--p graf-after--li"><em class="markup--em markup--p-em">Note: The decision trees in random forest can be built on a subset of data and features. Particularly, the sklearn model of random forest uses all features for decision tree and a subset of features are randomly selected for splitting at each node.</em></p><h3 name="dda5" id="dda5" class="graf graf--h3 graf-after--p">IMPORTANT HYPERPARAMETERS</h3><p name="146e" id="146e" class="graf graf--p graf-after--h3">The hyperparameters in random forest are either used to increase the predictive power of the model or to make the model faster. Let’s look at the hyperparameters of sklearns built-in random forest function.</p><ol class="postList"><li name="73c5" id="73c5" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Increasing Predictive power</strong></li></ol><p name="99c2" id="99c2" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">n_estimators:</strong></p><ul class="postList"><li name="6fe8" id="6fe8" class="graf graf--li graf-after--p">It defines the number of decision trees to be created in a random forest.</li><li name="bf84" id="bf84" class="graf graf--li graf-after--li">Generally, a higher number makes the predictions stronger and more stable, but a very large number can result in higher training time.</li></ul><p name="d4ee" id="d4ee" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">max_features</strong> :</p><ul class="postList"><li name="8728" id="8728" class="graf graf--li graf-after--p">It defines the maximum number of features allowed for the split in each decision tree.</li><li name="1130" id="1130" class="graf graf--li graf-after--li">Increasing max features usually improve performance but a very high number can decrease the diversity of each tree</li></ul><p name="ccf2" id="ccf2" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">min_samples_leaf:</strong></p><ul class="postList"><li name="a035" id="a035" class="graf graf--li graf-after--p">This defines the minimum number of samples required to be at a leaf node.</li><li name="e367" id="e367" class="graf graf--li graf-after--li">Smaller leaf size makes the model more prone to capturing noise in train data.</li></ul><p name="fee5" id="fee5" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">criterion</strong>:</p><ul class="postList"><li name="a009" id="a009" class="graf graf--li graf-after--p">It defines the function that is to be used for splitting.</li><li name="00aa" id="00aa" class="graf graf--li graf-after--li">The function measures the quality of a split for each feature and chooses the best split.</li></ul><p name="b584" id="b584" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">2. Increasing the model’s speed</strong></p><p name="ceb3" id="ceb3" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">max_depth</strong>:</p><ul class="postList"><li name="cbf2" id="cbf2" class="graf graf--li graf-after--p">Random forest has multiple decision trees. This parameter defines the maximum depth of the trees.</li></ul><p name="2a9a" id="2a9a" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">min_samples_split:</strong></p><ul class="postList"><li name="3c12" id="3c12" class="graf graf--li graf-after--p">Used to define the minimum number of samples required in a leaf node before a split is attempted.</li><li name="00c4" id="00c4" class="graf graf--li graf-after--li">If the number of samples is less than the required number, the node is not split</li></ul><p name="9f6c" id="9f6c" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">max_leaf_nodes:</strong></p><ul class="postList"><li name="ee6d" id="ee6d" class="graf graf--li graf-after--p">This parameter specifies the maximum number of leaf nodes for each tree.</li><li name="6afb" id="6afb" class="graf graf--li graf-after--li">The tree stops splitting when the number of leaf nodes becomes equal to the max leaf node.</li></ul><p name="3b54" id="3b54" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">n_jobs</strong>:</p><ul class="postList"><li name="9a7b" id="9a7b" class="graf graf--li graf-after--p">This indicates the number of jobs to run in parallel.</li><li name="5999" id="5999" class="graf graf--li graf-after--li">Set value to -1 if you want it to run on all cores in the system.</li></ul><p name="0337" id="0337" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">random_state</strong>:</p><ul class="postList"><li name="da69" id="da69" class="graf graf--li graf-after--p">This parameter is used to define the random selection.</li><li name="2e95" id="2e95" class="graf graf--li graf-after--li">It is used for comparison between various models.</li></ul><p name="75c2" id="75c2" class="graf graf--p graf-after--li">Now, let’s look at the code using Scikit learn.</p><pre name="de06" id="de06" class="graf graf--pre graf-after--p"># importing required libraries<br>import pandas as pd<br>from sklearn.ensemble import RandomForestClassifier<br>from sklearn.metrics import accuracy_score</pre><pre name="3206" id="3206" class="graf graf--pre graf-after--pre"># Now, we need to predict the missing target variable in the test data<br>#Create the object of the Random Forest model<br>#You can also add other parameters and test your code here<br>#Some parameters are : n_estimators and max_depth</pre><pre name="082e" id="082e" class="graf graf--pre graf-after--pre">model = RandomForestClassifier()</pre><pre name="ca6e" id="ca6e" class="graf graf--pre graf-after--pre"># fit the model with the training data<br>model.fit(train_x,train_y)</pre><pre name="e088" id="e088" class="graf graf--pre graf-after--pre"># number of trees used<br>print(&#39;Number of Trees used : &#39;, model.n_estimators)</pre><pre name="d71d" id="d71d" class="graf graf--pre graf-after--pre"># predict the target on the train dataset<br>predict_train = model.predict(train_x)<br>print(&#39;\nTarget on train data&#39;,predict_train)</pre><pre name="eedd" id="eedd" class="graf graf--pre graf-after--pre"># Accuray Score on train dataset<br>accuracy_train = accuracy_score(train_y,predict_train)<br>print(&#39;\naccuracy_score on train dataset : &#39;, accuracy_train)</pre><pre name="2de9" id="2de9" class="graf graf--pre graf-after--pre"># predict the target on the test dataset<br>predict_test = model.predict(test_x)<br>print(&#39;\nTarget on test data&#39;,predict_test)</pre><pre name="7ff1" id="7ff1" class="graf graf--pre graf-after--pre"># Accuracy Score on test dataset<br>accuracy_test = accuracy_score(test_y,predict_test)<br>print(&#39;\naccuracy_score on test dataset : &#39;, accuracy_test)</pre><h3 name="023e" id="023e" class="graf graf--h3 graf-after--pre">ADVANTAGES AND DISADVANTAGES OF THE RANDOM FOREST ALGORITHM</h3><p name="beb9" id="beb9" class="graf graf--p graf-after--h3">One of the biggest advantages of random forest is its versatility. It can be used for both regression and classification tasks, and it’s also easy to view the relative importance it assigns to the input features.</p><p name="92a8" id="92a8" class="graf graf--p graf-after--p">Random forest is also a very handy algorithm because the default hyperparameters it uses often produce a good prediction result. Understanding the hyperparameters is pretty straightforward, and there’s also not that many of them.</p><p name="4f95" id="4f95" class="graf graf--p graf-after--p">One of the biggest problems in machine learning is overfitting, but most of the time this won’t happen thanks to the random forest classifier. If there are enough trees in the forest, the classifier won’t overfit the model.</p><p name="a58f" id="a58f" class="graf graf--p graf-after--p">The main limitation of random forest is that a large number of trees can make the algorithm too slow and ineffective for real-time predictions. In general, these algorithms are fast to train, but quite slow to create predictions once they are trained. A more accurate prediction requires more trees, which results in a slower model. In most real-world applications, the random forest algorithm is fast enough but there can certainly be situations where run-time performance is important and other approaches would be preferred.</p><p name="62d7" id="62d7" class="graf graf--p graf-after--p">And, of course, random forest is a predictive modeling tool and not a descriptive tool, meaning if you’re looking for a description of the relationships in your data, other approaches would be better.</p><h3 name="ae67" id="ae67" class="graf graf--h3 graf-after--p">SUMMARY</h3><p name="d8ad" id="d8ad" class="graf graf--p graf-after--h3">Random forest is a great algorithm to train early in the model development process, to see how it performs. The algorithm is also a great choice for anyone who needs to develop a model quickly. On top of that, it provides a pretty good indicator of the importance it assigns to your features.</p><p name="a6bd" id="a6bd" class="graf graf--p graf-after--p">Random forests are also very hard to beat performance wise. Of course, you can probably always find a model that can perform better, like a neural network for example, but these usually take more time to develop, though they can handle a lot of different feature types, like binary, categorical and numerical.</p><p name="1343" id="1343" class="graf graf--p graf-after--p">Overall, random forest is a (mostly) <strong class="markup--strong markup--p-strong">fast</strong>, <strong class="markup--strong markup--p-strong">simple</strong> and <strong class="markup--strong markup--p-strong">flexible</strong> tool, but not without some limitations.</p><p name="0cc8" id="0cc8" class="graf graf--p graf-after--p graf--trailing">Thank you for reading my blog. Hope you understood it well. For more such blogs, make sure that you follow me. Cheers! Have a great day.</p></div></div></section>
</section><!--
<footer><p>By <a href="https://medium.com/@deeppatel23" class="p-author h-card">Deep Patel</a> on <a href="https://medium.com/p/ce01bef08152"><time class="dt-published" datetime="2020-06-29T06:54:37.633Z">June 29, 2020</time></a>.</p><p><a href="https://medium.com/@deeppatel23/random-forest-classifier-data-science-ml-part-7-3-ce01bef08152" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on September 4, 2020.</p></footer>--></article></body></html>